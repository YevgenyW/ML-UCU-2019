{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning. Assignment 1\n",
    "\n",
    "## Oleh Lukianykhin, Yevhen Pozdniakov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a.\n",
    "$$J(\\theta) = \\frac{1}{2}\\sum_{i=1}^{m}\\omega^{(i)}(\\theta^T x^{(i)} - y^{(i)})^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$X\\theta$ is the prediction for the given data, $X\\theta-y$ is vector-column of errors.\n",
    "The sum of squared errors:\n",
    "$$(X\\theta-y)^T(X\\theta-y)=\\sum_{i=1}^{m}(\\theta^T x^{(i)} - y^{(i)})^2$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $W$ be:\n",
    "$$W=\\begin{pmatrix}\\omega^{(1)}/2 & 0\\\\\n",
    "0 & \\omega^{(2)}/2 & \\dots\\\\\n",
    "\\dots & \\dots & \\dots \\\\\n",
    "\\dots & 0 & \\omega^{(m)}/2\\end{pmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then:\n",
    "$$(X\\theta-y)^TW(X\\theta-y)=\\sum_{i=1}^{m}(\\theta^T x^{(i)} - y^{(i)})\\frac{\\omega^{(i)}}{2}(\\theta^T x^{(i)} - y^{(i)}) = \n",
    " \\frac{1}{2}\\sum_{i=1}^{m}\\omega^{(i)}(\\theta^T x^{(i)} - y^{(i)})^2=J(\\theta)$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So:\n",
    "$$J(\\theta)=(X\\theta-y)^TW(X\\theta-y)$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b.\n",
    "$$\\frac{\\delta J}{\\delta \\theta_j} = \\sum_{i=1}^{m}\\omega^{(i)}(x^{(i)}\\theta - y^{(i)})x^{(i)}_j$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a matrix form for j-th coefficient it equals:\n",
    " $$\\frac{\\delta J}{\\delta \\theta_j} = X^T_{:,j}W(X\\theta - y),$$\n",
    " \n",
    "where $X_{:,j}$ denotes the j-th column of the object-feature matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then for the matrix form we have:\n",
    " $$\\nabla_{\\theta} J = X^TW(X\\theta - y),$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the required optimum gradient equals zero. Thus normal equation is given as follows:\n",
    " $$\\nabla_{\\theta} J = X^TW(X\\theta - y)=0,$$\n",
    " $$X^TWX\\theta = X^TWy,$$\n",
    " \n",
    " Then, analytical solution is:\n",
    " $$\\theta =(X^TWX)^{-1} X^TWy,$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a.\n",
    "Exponential family distributions are described by equation:\n",
    "$$P(y|\\eta)=b(y)exp(\\eta^TT(y) - a(\\eta))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's transform Poisson distribution to this form:\n",
    "$$P(y|\\lambda)=\\frac{e^{-\\lambda}\\lambda^y}{y!}=exp(ln(\\frac{e^{-\\lambda}\\lambda^y}{y!}))=exp(ln(e^{-\\lambda}) + ln(\\lambda^y) - ln(y!))=\\\\\n",
    "=\\frac{exp( y ln(\\lambda)-\\lambda)}{y!}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus:\n",
    "$$ \\eta=ln(\\lambda), \\quad a(\\eta) = \\lambda = e^{ln(\\lambda)} = e^{\\eta}, \\\\\n",
    "b(y) = \\frac{1}{y!},\\quad T(y) = y,\n",
    " $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b.\n",
    "The canonical response function is expectation of sufficient statistic of $y$ for the given value of parameter $\\eta$:\n",
    "$$g(\\eta)=E(T(y)|\\eta)=E(y|\\eta)$$\n",
    "\n",
    "As we know, for the Poisson distribution:\n",
    "$$E(y|\\lambda)=\\lambda$$\n",
    "\n",
    "Thus canonical response function equals:\n",
    "$$g(\\eta)=E(T(y)|\\eta)=\\lambda=e^{\\eta}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c.\n",
    "Third assumptions for GLM:\n",
    "$$\\eta = \\theta^T x$$\n",
    "$$P(y|\\eta)=\\frac{exp(\\eta^T y-exp(\\eta))}{y!}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus:\n",
    "$$P(y^{(i)}|x^{(i)},\\theta)=\\frac{1}{y^{(i)}!}exp\\left((x^{(i)})^T\\theta y^{(i)} - exp(\\theta^T x^{(i)})\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Likelihood function is:\n",
    "$$L=\\prod_{i=1}^m P(y^{(i)}|x^{(i)},\\theta)=\\prod_{i=1}^m\\frac{1}{y^{(i)}!}exp\\left((x^{(i)})^T\\theta y^{(i)} - exp(\\theta^T x^{(i)})\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then log-likelihood is given as follows:\n",
    "$$l=ln(L)=ln\\prod_{i=1}^m P(y^{(i)}|x^{(i)},\\theta)=\\sum_{i=1}^mln\\left(\\frac{1}{y^{(i)}!}exp((x^{(i)})^T\\theta y^{(i)} - exp(\\theta^T x^{(i)})) \\right)=\\sum_{i=1}^m ln\\left(exp\\left((x^{(i)})^T\\theta y^{(i)} - exp(\\theta^T x^{(i)})\\right)-ln(y^{(i)}!) \\right)= \\\\\n",
    "=\\sum_{i=1}^m (x^{(i)})^T\\theta y^{(i)} - exp(\\theta^T x^{(i)})-ln(y^{(i)}!) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's derivative:\n",
    "$$\\frac{\\delta l}{\\delta \\theta_j} = \\sum_{i=1}^{m}x^{(i)}_j(y^{(i)}-e^{\\theta^T x^{(i)}})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then stochastic gradient ascend will be described as follows:\n",
    "* on each step choose randomly one object (i) from the training sample\n",
    "* then update coefficients $\\theta_j$ using the following formula:\n",
    "$$\\theta_j=\\theta_j+\\alpha \\left( x^{(i)}_j(y^{(i)}-e^{\\theta^T x^{(i)}})\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rewrite standart formula for linear regression model with weighted coefficients $\\omega$:\n",
    "$$P(y|x;\\theta) = (h_{\\theta}(x))^{\\omega y}(1-h_{\\theta}(x))^{\\omega(1-y)}.$$\n",
    "With m trainee examples we can write maximimum likelihood function as:\n",
    "$$L(\\theta) = \\displaystyle\\prod_{i=1}^{m} P(y^{(i)}|x^{(i)},\\theta) = \\displaystyle\\prod_{i=1}^{m} (h_{\\theta}(x^{(i)}))^{\\omega^{(i)} y^{(i)}}(1-h_{\\theta}(x^{(i)}))^{\\omega^{(i)}(1-y^{(i)})}$$\n",
    "Logarithm of maximum likelihood function:\n",
    "$$l(\\theta) = log(L(\\theta)) = \\sum_{i=1}^{m} \\omega^{(i)} [y^{(i)}log(h_{\\theta}(x^{(i)}) + (1-y^{(i)}) log(1-h_{\\theta}(x^{(i)}))]$$\n",
    "Log-loss function is a inverse function to log-likelihood function:\n",
    "$$J(\\theta) = -\\frac{1}{m}l(\\theta) = -\\frac{1}{m}\\sum_{i=1}^{m} \\omega^{(i)} [y^{(i)}log(h_{\\theta}(x^{(i)}) + (1-y^{(i)}) log(1-h_{\\theta}(x^{(i)}))]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a.alt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the weighted modification of algorithm at each step we consider some point $x_{considered}$, assign weights to all observations available in our data using the following formula:\n",
    "\n",
    "$$\\omega^{(i)}=exp\\left( -\\frac{(x^{(i)}-x_{considered})^T(x^{(i)}-x_{considered})}{2\\tau^2} \\right)$$\n",
    "\n",
    "Or in a matrix form:\n",
    "\n",
    "$$ W=diag(exp\\left( -\\frac{(X^T-x_{considered})^T(X^T-x_{considered})}{2\\tau^2} \\right))$$\n",
    "\n",
    "Then we use this weights to get cost-function specifically for the case of $x_{considered}$:\n",
    "\n",
    "$$J_{considered}(\\theta)= \\frac{1}{m}\\sum_{i=1}^{m}err(y^{(i)})\\omega^{(i)},$$\n",
    "\n",
    "where $err(y^{(i)})$ is the error introduced by model prediction at $i$-th point of the data, i.e. it is the difference between predicted and real value calculated in certain way. For MSE it would be squared difference.\n",
    "\n",
    "For the logistic regression log-loss cost function is its' log-likelihood multiplied by -1. Thus error term is as follows:\n",
    "\n",
    "$$ err(i) =-\\left( y^{(i)}log(h_{\\theta}(x^{(i)}) + (1-y^{(i)}) log(1-h_{\\theta}(x^{(i)}))\\right),$$\n",
    "\n",
    "where $h_{\\theta}(x^{(i)}$ denotes hypothesis for the given $x^{(i)}$ and $\\theta$. In this case it's sigmoid function.\n",
    "\n",
    "Log-loss cost function for the weighted logistic regression is given as follows:\n",
    "\n",
    "$$J(\\theta) = -\\frac{1}{m}l(\\theta) = -\\frac{1}{m}\\sum_{i=1}^{m} \\omega^{(i)} \\left(y^{(i)}log(h_{\\theta}(x^{(i)}) + (1-y^{(i)}) log(1-h_{\\theta}(x^{(i)}))\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To minimize the cost function function, let's use gradient descent:\n",
    "$$\\theta^{k+1} = \\theta^{k} - \\alpha \\cdot \\nabla_{\\theta}J(\\theta)$$\n",
    "In our case:\n",
    "$$h_{\\theta}(x) = g(z) = \\frac{1}{1+e^{-z}},\\space z = \\theta^{T}x$$\n",
    "$$g(z)^{'} = g(z)(1 - g(z))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Partial derivative of the cost function by $j$-th parameter of the model - $\\theta_j$:\n",
    "$$\\frac{\\partial J(\\theta)}{\\partial \\theta_j} = -\\frac{1}{m}(\\sum_{i=1}^{m}\\frac{\\partial}{\\partial \\theta_j}\\left( \\omega^{(i)} \\left(y^{(i)}log(g(z)) + (1-y^{(i)}) log(1-g(z))\\right)\\right) =\\\\=\n",
    "-\\frac{1}{m}\\sum_{i=1}^{m}\\omega^{(i)}\\left(y^{(i)}\\frac{1}{g(z)}\\frac{\\partial }{\\partial \\theta_j}g(z) + (1-y^{(i)})\\frac{1}{1-g(z)}\\frac{\\partial }{\\partial \\theta_j}(1 - g(z))\\right) =\\\\= \n",
    "-\\frac{1}{m}\\sum_{i=1}^{m}\\omega^{(i)}\\left(y^{(i)}(1-g(z)) - (1-y^{(i)})g(z)\\right) \\frac{\\partial }{\\partial \\theta_j}z =\\\\=\n",
    "-\\frac{1}{m}\\sum_{i=1}^{m}\\omega^{(i)} \\left(y^{(i)}-y^{(i)}g(z)-g(z) + y^{(i)}g(z)\\right)\\frac{\\partial }{\\partial \\theta_j}(\\theta^Tx^{(i)}) =\\\\=\n",
    "-\\frac{1}{m}\\sum_{i=1}^{m}\\omega^{(i)}x^{(i)}_j(y^{(i)} - g(z)) =  -\\frac{1}{m}\\sum_{i=1}^{m}\\omega^{(i)}x^{(i)}_j(y^{(i)} - h_{\\theta}(x^{(i)}))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We receive the following formula to update $\\theta$'s at ech step of the gradient descent method:\n",
    "$$\\theta_j^{(k+1)} = \\theta_j^{(k)} +\\frac{1}{m} \\alpha\\cdot \\sum_{i=1}^{m}(y^{(i)} - h_{\\theta}(x^{(i)})x_j^{(i)}\\omega^{(i)}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would like to find analytical solution for a weighted logistic regression using normal equations.\n",
    "\n",
    "To this end, we write down the condition of solution - the gradient of cost function equals zero.\n",
    "\n",
    "Writing this in the matrix form we receive equation that leads us to the required solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient of the weighted logisitc regression was derived in the paragraph b. Let's write it down in the matrix form:\n",
    "$$\\frac{\\partial J(\\theta)}{\\partial \\theta_j} =  -\\frac{1}{m}\\sum_{i=1}^{m}\\omega^{(i)}x^{(i)}_j(y^{(i)} - h_{\\theta}(x^{(i)})) = X^TW(y-h_{\\theta}(X)),$$\n",
    "where m - number of observation, y - vector of results (m $\\times$ 1), X - matrix of obervations (m $\\times$ n), $\\omega$ - diagonal matrix of weights (m $\\times$ m), $h$ - hypothesis function. In our case $h_{\\theta}(X)=\\frac{1}{1+exp(-X\\theta)}$ - sigmoid."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then required equation is given as follows:\n",
    "$$\\frac{\\partial J(\\theta)}{\\partial \\theta_j} = X^TW(y-h_{\\theta}(X)) = 0$$\n",
    "$$X^TWy = X^TWh_{\\theta}(X)=X^TW\\frac{1}{1+exp(-X\\theta)} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, the required equation is as follows:\n",
    "$$X^TWy = X^TW\\frac{1}{1+exp(-X\\theta)} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, it is non-linear in $\\theta$'s and . One should choose aproach aplicable to the considered case to get the desired solution, e.g. particular numerical method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For instance, if we skip linearly dependent part of $X^TW$ and find it's left-pseudo inverse, we will be able to move to the following form:\n",
    "$$\\hat y = \\frac{1}{1+exp(-\\hat X\\hat \\theta)} ,$$\n",
    "\n",
    "it leadds us to the following solution:\n",
    "$$\\hat \\theta = \\hat X ^{-1*}(ln(\\hat y)-ln(1-\\hat y)),$$\n",
    "\n",
    "where $\\hat X ^{-1*}$ is an inverse or a pseudoinverse, depending on what exist for the $\\hat X$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A model that expects speed in kph (kilometers per hour):\n",
    "\n",
    "$$y_1 = \\theta_0 + \\theta_1 x_1 +  \\theta_2 x_2 +  \\theta_3 x_3,$$\n",
    "\n",
    "where $x_3$ - speed (in kph)\n",
    "\n",
    "As we know, 1 mile $\\approx 1.6$ km, so the coefficient $k = 1.6.$\n",
    "\n",
    "The modified model that expects speed in mph (miles per hour):\n",
    "\n",
    "$$y_2 = \\theta_0 + \\theta_1 x_1 +  \\theta_2 x_2 +  \\theta_3 k x_3.$$\n",
    "\n",
    "I consider a microchip as a \"black box\" (no information about value of the variables $x_3$ and $\\theta_3$). \n",
    "\n",
    "We can write the following:\n",
    "\n",
    "$$y_2 = (y_1 - (\\theta_0 + \\theta_1x_1 + \\theta_2x_2))\\cdot k + (\\theta_0 + \\theta_1x_1 + \\theta_2x_2) = (y_1 - \\tilde{y})\\cdot k + \\tilde{y},$$\n",
    "\n",
    "where $\\tilde{y}$ - result of the model $y_1$ in case of input feature vector $(x_1,\\space x_2, \\space 0)^T$.\n",
    "\n",
    "Finally, our algorithm for every input feature vector $(x_1,\\space x_2, \\space x_3)$:\n",
    "1. Get input feature vector $(x_1,\\space x_2, \\space x_3) \\rightarrow$ calculate $y_1$\n",
    "2. Get input feature vector $(x_1,\\space x_2, \\space 0) \\rightarrow$ calculate $\\tilde{y}$\n",
    "3. $y_2 = (y_1 - \\tilde{y})\\cdot k + \\tilde{y}$ - result of the model that takes into account miles instead of kilometers.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
