{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a.\n",
    "$$J(\\theta) = \\frac{1}{2}\\sum_{i=1}^{m}\\omega^{(i)}(\\theta^T x^{(i)} - y^{(i)})^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$X\\theta$ is the prediction for the given data, $X\\theta-y$ is vector-column of errors.\n",
    "The sum of squared errors:\n",
    "$$(X\\theta-y)^T(X\\theta-y)=\\sum_{i=1}^{m}(\\theta^T x^{(i)} - y^{(i)})^2$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $W$ be:\n",
    "$$W=\\begin{pmatrix}\\omega^{(1)}/2 & 0\\\\\n",
    "0 & \\omega^{(2)}/2 & \\dots\\\\\n",
    "\\dots & \\dots & \\dots \\\\\n",
    "\\dots & 0 & \\omega^{(m)}/2\\end{pmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then:\n",
    "$$(X\\theta-y)^TW(X\\theta-y)=\\sum_{i=1}^{m}(\\theta^T x^{(i)} - y^{(i)})\\frac{\\omega^{(i)}}{2}(\\theta^T x^{(i)} - y^{(i)}) = \n",
    " \\frac{1}{2}\\sum_{i=1}^{m}\\omega^{(i)}(\\theta^T x^{(i)} - y^{(i)})^2=J(\\theta)$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So:\n",
    "$$J(\\theta)=(X\\theta-y)^TW(X\\theta-y)$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b.\n",
    "$$\\frac{\\delta J}{\\delta \\theta_j} = \\sum_{i=1}^{m}\\omega^{(i)}(x^{(i)}\\theta - y^{(i)})x^{(i)}_j$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a matrix form for j-th coefficient it equals:\n",
    " $$\\frac{\\delta J}{\\delta \\theta_j} = X^T_{:,j}W(X\\theta - y),$$\n",
    " \n",
    "where $X_{:,j}$ denotes the j-th column of the object-feature matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then for the matrix form we have:\n",
    " $$\\nabla_{\\theta} J = X^TW(X\\theta - y),$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the required optimum gradient equals zero. Thus normal equation is given as follows:\n",
    " $$\\nabla_{\\theta} J = X^TW(X\\theta - y)=0,$$\n",
    " $$X^TWX\\theta = X^TWy,$$\n",
    " \n",
    " Then, analytical solution is:\n",
    " $$\\theta =(X^TWX)^{-1} X^TWy,$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a.\n",
    "Exponential family distributions are described by equation:\n",
    "$$P(y|\\eta)=b(y)exp(\\eta^TT(y) - a(\\eta))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's transform Poisson distribution to this form:\n",
    "$$P(y|\\lambda)=\\frac{e^{-\\lambda}\\lambda^y}{y!}=exp(ln(\\frac{e^{-\\lambda}\\lambda^y}{y!}))=exp(ln(e^{-\\lambda}) + ln(\\lambda^y) - ln(y!))=\\\\\n",
    "=\\frac{exp( y ln(\\lambda)-\\lambda)}{y!}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus:\n",
    "$$ \\eta=ln(\\lambda), \\quad a(\\eta) = \\lambda = e^{ln(\\lambda)} = e^{\\eta}, \\\\\n",
    "b(y) = \\frac{1}{y!},\\quad T(y) = y,\n",
    " $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b.\n",
    "The canonical response function is expectation of sufficient statistic of $y$ for the given value of parameter $\\eta$:\n",
    "$$g(\\eta)=E(T(y)|\\eta)=E(y|\\eta)$$\n",
    "\n",
    "As we know, for the Poisson distribution:\n",
    "$$E(y|\\lambda)=\\lambda$$\n",
    "\n",
    "Thus canonical response function equals:\n",
    "$$g(\\eta)=E(T(y)|\\eta)=\\lambda=e^{\\eta}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c.\n",
    "Third assumptions for GLM:\n",
    "$$\\eta = \\theta^T x$$\n",
    "$$P(y|\\eta)=\\frac{exp(\\eta^T y-exp(\\eta))}{y!}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus:\n",
    "$$P(y^{(i)}|x^{(i)},\\theta)=\\frac{1}{y^{(i)}!}exp\\left((x^{(i)})^T\\theta y^{(i)} - exp(\\theta^T x^{(i)})\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Likelihood function is:\n",
    "$$L=\\prod_{i=1}^m P(y^{(i)}|x^{(i)},\\theta)=\\prod_{i=1}^m\\frac{1}{y^{(i)}!}exp\\left((x^{(i)})^T\\theta y^{(i)} - exp(\\theta^T x^{(i)})\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then log-likelihood is given as follows:\n",
    "$$l=ln(L)=ln\\prod_{i=1}^m P(y^{(i)}|x^{(i)},\\theta)=\\sum_{i=1}^mln\\left(\\frac{1}{y^{(i)}!}exp((x^{(i)})^T\\theta y^{(i)} - exp(\\theta^T x^{(i)})) \\right)=\\sum_{i=1}^m ln\\left(exp\\left((x^{(i)})^T\\theta y^{(i)} - exp(\\theta^T x^{(i)})\\right)-ln(y^{(i)}!) \\right)= \\\\\n",
    "=\\sum_{i=1}^m (x^{(i)})^T\\theta y^{(i)} - exp(\\theta^T x^{(i)})-ln(y^{(i)}!) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's derivative:\n",
    "$$\\frac{\\delta l}{\\delta \\theta_j} = \\sum_{i=1}^{m}x^{(i)}_j(y^{(i)}-e^{\\theta^T x^{(i)}})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then stochastic gradient ascend will be described as follows:\n",
    "* on each step choose randomly one object (i) from the training sample\n",
    "* then update coefficients $\\theta_j$ using the following formula:\n",
    "$$\\theta_j=\\theta_j+\\alpha \\left( x^{(i)}_j(y^{(i)}-e^{\\theta^T x^{(i)}})\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rewrite standart formula for linear regression model with weighted coefficients $\\omega$:\n",
    "$$P(y|x;\\theta) = (h_{\\theta}(x))^{\\omega y}(1-h_{\\theta}(x))^{\\omega(1-y)}.$$\n",
    "With m trainee examples we can write maximimum likelihood function as:\n",
    "$$L(\\theta) = \\displaystyle\\prod_{i=1}^{m} P(y^{(i)}|x^{(i)},\\theta) = \\displaystyle\\prod_{i=1}^{m} (h_{\\theta}(x^{(i)}))^{\\omega^{(i)} y^{(i)}}(1-h_{\\theta}(x^{(i)}))^{\\omega^{(i)}(1-y^{(i)})}$$\n",
    "Logarithm of maximum likelihood function:\n",
    "$$l(\\theta) = log(L(\\theta)) = \\sum_{i=1}^{m} \\omega^{(i)} [y^{(i)}log(h_{\\theta}(x^{(i)}) + (1-y^{(i)}) log(1-h_{\\theta}(x^{(i)}))]$$\n",
    "Log-loss function is a inverse function to log-likelihood function:\n",
    "$$J(\\theta) = -\\frac{1}{m}l(\\theta) = -\\frac{1}{m}\\sum_{i=1}^{m} \\omega^{(i)} [y^{(i)}log(h_{\\theta}(x^{(i)}) + (1-y^{(i)}) log(1-h_{\\theta}(x^{(i)}))]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To maximixe log-likelihood function, let's use gradient descent:\n",
    "$$\\theta^{k+1} = \\theta^{k} + \\alpha \\cdot \\nabla_{\\theta}l(\\theta)$$\n",
    "Some standart definitions:\n",
    "$$h_{\\theta}(x) = g(z) = \\frac{1}{1+e^{-z}},\\space z = \\theta^{T}x$$\n",
    "$$g(z)^{'} = g(z)(1 - g(z))$$\n",
    "Partial derivative:\n",
    "$$\\frac{\\partial l(\\theta)}{\\partial \\theta} = (\\sum_{i=1}^{m} \\omega^{(i)} [y^{(i)}log(g(z)) + (1-y^{(i)}) log(1-g(z))])^{'} = \\sum_{i=1}^{m}\\omega^{(i)}[y^{(i)}\\frac{1}{g(z)}g(z)^{'} + (1-y^{(i)})\\frac{1}{1-g(z)}(1 - g(z))^{'}] = \\sum_{i=1}^{m}\\omega^{(i)}[y^{(i)}(1-g(z)) - (1-y)g(z)]z^{'} = \\sum_{i=1}^{m}\\omega^{(i)} (\\theta^Tx)[y^{(i)}-y^{(i)}g(z)-g(z) + y^{(i)}g(z)] = \\sum_{i=1}^{m}\\omega^{(i)}x(y^{(i)} - g(z)) =  \\sum_{i=1}^{m}\\omega^{(i)}x(y^{(i)} - h_{\\theta}(x^{(i)}))$$\n",
    "Finally, formula for update $\\theta$ in gradient descent method:\n",
    "$$\\theta_j^{(k+1)} = \\theta_j^{(k)} - \\alpha\\cdot \\sum_{i=1}^{m}(y^{(i)} - h_{\\theta}(x^{(i)})x_j^{(i)}\\omega^{(i)}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A model that expects speed in kph (kilometers per hour):\n",
    "$$y_1 = \\theta_0 + \\theta_1 x_1 +  \\theta_2 x_2 +  \\theta_3 x_3,$$\n",
    "where $x_3$ - speed (in kph)\n",
    "<br> As we know, 1 mile $\\approx 1.6$ km, so the coefficient $k = 1.6.$\n",
    "<br> The modified model that expects speed in mph (miles per hour):\n",
    "$$y_2 = \\theta_0 + \\theta_1 x_1 +  \\theta_2 x_2 +  \\theta_3 k x_3.$$\n",
    "<br> I consider a microchip as a \"black box\" (no information about value of the variables $x_3$ and $\\theta_3$). \n",
    "<br> We can write the following:\n",
    "$$y_2 = (y_1 - (\\theta_0 + \\theta_1x_1 + \\theta_2x_2))\\cdot k + (\\theta_0 + \\theta_1x_1 + \\theta_2x_2) = (y_1 - \\tilde{y})\\cdot k + \\tilde{y},$$\n",
    "where $\\tilde{y}$ - result of the model $y_1$ in case of input feature vector $(x_1,\\space x_2, \\space 0)^T$.\n",
    "<br> Finally, our algorithm for every input feature vector $(x_1,\\space x_2, \\space x_3)$:\n",
    "<br> 1. Get input feature vector $(x_1,\\space x_2, \\space x_3) \\rightarrow$ calculate $y_1$\n",
    "<br> 2. Get input feature vector $(x_1,\\space x_2, \\space 0) \\rightarrow$ calculate $\\tilde{y}$\n",
    "<br> 3. $y_2 = (y_1 - \\tilde{y})\\cdot k + \\tilde{y}$ - result of the model that takes into account miles instead of kilometers.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
