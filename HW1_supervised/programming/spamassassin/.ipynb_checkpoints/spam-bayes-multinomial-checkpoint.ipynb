{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Класифікація спаму: Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Мультиноміальна модель подій**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ми застосуємо наївний Баєсівський класифікатор зі згладжуванням Лапласа для навчання спам-фільтру на основі даних [SpamAssassin Public Corpus](http://spamassassin.apache.org/publiccorpus/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Заповніть пропущений код (позначено коментарями) та визначте точність передбачення. У вас повинен вийти кращий результат, ніж при моделі багатовимірного розподілу Бернуллі."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-16T15:20:23.838374Z",
     "start_time": "2019-02-16T15:20:23.835352Z"
    }
   },
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-16T15:20:23.844606Z",
     "start_time": "2019-02-16T15:20:23.841539Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm_notebook as progressbar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Завантаження даних"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Щоб краще зрозуміти, яким чином були очищені дані, див. [`spam-data-preparation.ipynb`](spam-data-preparation.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-16T15:20:23.849931Z",
     "start_time": "2019-02-16T15:20:23.847123Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_json_from_file(filename):\n",
    "    with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-16T15:20:24.140298Z",
     "start_time": "2019-02-16T15:20:23.852473Z"
    }
   },
   "outputs": [],
   "source": [
    "emails_tokenized_ham = load_json_from_file(\"emails-tokenized-ham.json\")\n",
    "emails_tokenized_spam = load_json_from_file(\"emails-tokenized-spam.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-16T15:20:24.167878Z",
     "start_time": "2019-02-16T15:20:24.142153Z"
    }
   },
   "outputs": [],
   "source": [
    "vocab = load_json_from_file(\"vocab.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Кодування даних"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Представте кожен лист як $n$-вимірний вектор $\\left[ x_1, x_2, ..., x_n \\right]$, де $x_i$ — це індекс $i$-го слова даного листа у словнику $V$, а $n$ — кількість слів у листі.\n",
    "\n",
    "Наприклад, лист _\"Buy gold watches. Buy now.\"_ міг би бути закодований так: $\\left[ 3953, 11890, 32213, 3953, 20330 \\right]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-16T15:20:24.178043Z",
     "start_time": "2019-02-16T15:20:24.169915Z"
    }
   },
   "outputs": [],
   "source": [
    "def email_to_vector_multinomial(email_words, vocab):\n",
    "    # =============== TODO: Your code here ===============\n",
    "    # Build a feature vector for a single email using the\n",
    "    # multinomial event model.\n",
    "    res = np.zeros(len(email_words), dtype=\"int64\")\n",
    "    for i in range(len(email_words)):\n",
    "        res[i] = vocab[email_words[i]]\n",
    "    return res\n",
    "    # ===================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тепер закодуємо всі листи:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-16T15:20:24.631632Z",
     "start_time": "2019-02-16T15:20:24.180362Z"
    }
   },
   "outputs": [],
   "source": [
    "X = [\n",
    "    email_to_vector_multinomial(email, vocab)\n",
    "    for email in emails_tokenized_ham + emails_tokenized_spam\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-16T15:20:24.638006Z",
     "start_time": "2019-02-16T15:20:24.634563Z"
    }
   },
   "outputs": [],
   "source": [
    "y = np.array([0] * len(emails_tokenized_ham) + [1] * len(emails_tokenized_spam))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поглянемо на кілька випадкових листів:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-16T15:20:24.644596Z",
     "start_time": "2019-02-16T15:20:24.641947Z"
    }
   },
   "outputs": [],
   "source": [
    "sample_emails = [emails_tokenized_ham[10], emails_tokenized_ham[70]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-16T15:20:24.651190Z",
     "start_time": "2019-02-16T15:20:24.646522Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', 'seen', 'discuss', 'articl', 'approach', 'thank', 'httpaddress', 'hell', 'rule', 'tri', 'accomplish', 'someth', 'thoma', 'alva', 'edison', 'sf', 'net', 'email', 'sponsor', 'osdn', 'tire', 'old', 'cell', 'phone', 'get', 'new', 'free', 'httpaddress', 'spamassassin', 'devel', 'mail', 'list', 'emailaddress', 'httpaddress']\n",
      "\n",
      "['fri', 'number', 'aug', 'number', 'tom', 'wrote', 'xvid', 'number', 'project', 'make', 'gpl', 'divx', 'codec', 'sigma', 'design', 'number', 'sorri', 'sigma', 'design', 'number', 'number', 'httpaddress']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for email in sample_emails:\n",
    "    print(email)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-16T15:20:24.658275Z",
     "start_time": "2019-02-16T15:20:24.653328Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Email vector: [12866 26186  7632  1574  1361 29410 13468 12862 25408 30214   173 27396\n",
      " 29564   872  8567 26411 19758  8849 27722 21116 29770 20748  4549 22215\n",
      " 11528 19855 10871 13468 27540  7314 17535 16947  8851 13468]\n",
      "Dimensionality: (34,)\n",
      "\n",
      "Email vector: [10946 20419  1845 20419 29891 33008 33256 20419 23298 17594 12021  7779\n",
      "  5370 26756  7232 20419 27443 26756  7232 20419 20419 13468]\n",
      "Dimensionality: (22,)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for email in sample_emails:\n",
    "    email_vec = email_to_vector_multinomial(email, vocab)\n",
    "    \n",
    "    print(\"Email vector:\", email_vec)\n",
    "    print(\"Dimensionality:\", email_vec.shape)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Розділення вибірок"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-16T15:20:24.666633Z",
     "start_time": "2019-02-16T15:20:24.660884Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-16T15:20:24.671474Z",
     "start_time": "2019-02-16T15:20:24.668148Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Train: 4987\n",
      "# Test:  555\n"
     ]
    }
   ],
   "source": [
    "print(\"# Train:\", len(X_train))\n",
    "print(\"# Test: \", len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Навчання наївного Баєсового класифікатора"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Підрахуйте сумарну кількість слів у ham- і spam-листах відповідно."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-16T15:20:24.684269Z",
     "start_time": "2019-02-16T15:20:24.674510Z"
    }
   },
   "outputs": [],
   "source": [
    "# =============== TODO: Your code here ===============\n",
    "# Count the total number of words in ham and spam emails.\n",
    "# Store these counts into the two variables defined below.\n",
    "# print(len(y_train))\n",
    "# print(sum(num == 1 for num in y_train))\n",
    "# print(sum(num == 0 for num in y_train))\n",
    "\n",
    "# ham_total_words_train = sum(num == 0 for num in y_train)\n",
    "# spam_total_words_train = sum(num == 1 for num in y_train)\n",
    "\n",
    "ham_total_words_train = 0\n",
    "spam_total_words_train = 0\n",
    "\n",
    "for i in range(len(X_train)):\n",
    "    if (y_train[i] == 1):\n",
    "        spam_total_words_train += len(X_train[i]) \n",
    "    else:\n",
    "        ham_total_words_train += len(X_train[i]) \n",
    "# ===================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тепер обчисліть апріорні імовірності для класів ham і spam. Зауважте, що добуток імовірностей може переповнити тип даних змінної, тому ми будемо використовувати логарифми."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-16T15:20:24.693090Z",
     "start_time": "2019-02-16T15:20:24.686686Z"
    }
   },
   "outputs": [],
   "source": [
    "# =============== TODO: Your code here ===============\n",
    "# Compute the class priors for ham and spam emails.\n",
    "spam_total_emails_train = 0\n",
    "ham_total_emails_train = 0\n",
    "\n",
    "for value in y_train:\n",
    "    if (value == 1):\n",
    "        spam_total_emails_train+=1\n",
    "    else:\n",
    "        ham_total_emails_train+=1\n",
    "\n",
    "ham_log_prior = np.log(ham_total_emails_train/len(X_train))\n",
    "spam_log_prior = np.log(spam_total_emails_train/len(X_train))\n",
    "# ===================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обчисліть правдоподібності (likelihood) для кожного слова. Також, застосуйте згладжування Лапласа, щоб уникнути ділення на нуль."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Створимо порожні вектори $\\log{\\phi_{word \\, | \\, ham}}$ та $\\log{\\phi_{word \\, | \\, spam}}$ і заповнимо їх для кожного слова зі словника."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-16T15:20:24.699429Z",
     "start_time": "2019-02-16T15:20:24.695584Z"
    }
   },
   "outputs": [],
   "source": [
    "ham_log_phi = np.zeros(len(vocab), dtype=\"float64\")\n",
    "spam_log_phi = np.zeros(len(vocab), dtype=\"float64\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-16T15:20:24.703855Z",
     "start_time": "2019-02-16T15:20:24.701427Z"
    }
   },
   "outputs": [],
   "source": [
    "# ham_word_counts = np.zeros(len(vocab))\n",
    "# spam_word_counts = np.zeros(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-16T15:20:26.363119Z",
     "start_time": "2019-02-16T15:20:24.705704Z"
    }
   },
   "outputs": [],
   "source": [
    "# =============== TODO: Your code here ===============\n",
    "# Compute log phi(word | class) for each word in the vocabulary.\n",
    "# Fill out the `ham_log_phi` and `spam_log_phi` arrays below.\n",
    "ham_log_phi.fill(0)\n",
    "spam_log_phi.fill(0)\n",
    "\n",
    "for i in range(len(X_train)):\n",
    "    for word in X_train[i]:\n",
    "        if (y_train[i] == 1):\n",
    "            spam_log_phi[int(word)] += 1 \n",
    "        else:\n",
    "            ham_log_phi[int(word)] += 1 \n",
    "    \n",
    "spam_log_phi=np.log((spam_log_phi+1)/(spam_total_words_train+2))\n",
    "ham_log_phi=np.log((ham_log_phi+1)/(ham_total_words_train+2))\n",
    "\n",
    "# ===================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Передбачення"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реалізуйте функцію передбачення. Пригадайте, що знаменник $P(words)$ — один і той самий для обох класів, тому для передбачення його можна проігнорувати."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-16T15:20:26.370321Z",
     "start_time": "2019-02-16T15:20:26.365278Z"
    }
   },
   "outputs": [],
   "source": [
    "def predict(X):\n",
    "    # =============== TODO: Your code here ===============\n",
    "    # Implement the prediction of target classes, given\n",
    "    # a feature dataset X. You should return a response\n",
    "    # vector containing n {0, 1} values, where n is the\n",
    "    # number of examples in X.\n",
    "    \n",
    "    y_hat = np.zeros(len(X))\n",
    "    \n",
    "    for i in range(len(X)):\n",
    "        spam = spam_log_prior\n",
    "        ham = ham_log_prior\n",
    "        for word in X[i]:\n",
    "            spam += (spam_log_phi[int(word)])\n",
    "            ham += (ham_log_phi[int(word)])\n",
    "\n",
    "        if (spam >= ham):\n",
    "            y_hat[i] = 1\n",
    "        else:\n",
    "            y_hat[i] = 0            \n",
    "    return y_hat\n",
    "# ===================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Оцінка точності передбачення"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-16T15:20:27.738747Z",
     "start_time": "2019-02-16T15:20:26.372582Z"
    }
   },
   "outputs": [],
   "source": [
    "pred_train = predict(X_train)\n",
    "pred_test = predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-16T15:20:27.744434Z",
     "start_time": "2019-02-16T15:20:27.740773Z"
    }
   },
   "outputs": [],
   "source": [
    "accuracy_train = 1 - np.sum(pred_train != y_train) / len(y_train)\n",
    "accuracy_test = 1 - np.sum(pred_test != y_test) / len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-16T15:20:27.750141Z",
     "start_time": "2019-02-16T15:20:27.746453Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy:   97.694%\n",
      "Test accuracy:       97.658%\n"
     ]
    }
   ],
   "source": [
    "print(\"Training accuracy:   {0:.3f}%\".format(accuracy_train * 100))\n",
    "print(\"Test accuracy:       {0:.3f}%\".format(accuracy_test * 100))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
