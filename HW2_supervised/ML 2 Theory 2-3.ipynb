{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning\n",
    "# Assignment 2\n",
    "# Task 2\n",
    "Let $K_1$ describes projection from $x$ to $\\phi_1(x)$ and $K_1$ describes projection from $\\phi_1(x)$ to $\\phi_2(x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a.\n",
    "Let $K_1,K_2$ be linear kernels:\n",
    "$$K_1=\\alpha_1 x^Tz+c_1, \\qquad K_2=\\alpha_2 x^Tz+c_2$$\n",
    "\n",
    "Let's consder $K$ that describes projection from $x$ space to $\\phi_2(\\phi_1(x))$. By definition:\n",
    "\n",
    "$$K(x;z)=\\phi_2(\\phi_1(x))^T\\phi_2(\\phi_1(z))=K_2(\\phi_1(x);\\phi_1(z))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then:\n",
    "    $$K(x;z)=\\phi_2(\\phi_1(x))^T\\phi_2(\\phi_1(z))=K_2(\\phi_1(x);\\phi_1(z))=\\alpha_2 \\phi_1(x)^T\\phi_1(z)+c_2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition:\n",
    "    $$ \\phi_1(x)^T\\phi_1(z) = K_1(x;z) =  \\alpha_1 x^Tz+c_1$$\n",
    "    \n",
    "Thus:\n",
    "    $$K(x;z)=\\alpha_2 (\\alpha_1 x^Tz+c_1) +c_2=\\alpha_2\\alpha_1 x^Tz+(\\alpha_2c_1 +c_2)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the kernel for projection from space of $x$'s to space $\\phi_2(\\phi_1(x))$ is still linear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b.\n",
    "Let $K_1,K_2$ be polinomial kernels:\n",
    "$$K_1=(\\alpha_1 x^Tz+c_1)^{d_1}, \\qquad K_2=(\\alpha_2 x^Tz+c_2)^{d_2}$$\n",
    "\n",
    "Let's consder $K$ that describes projection from $x$ space to $\\phi_2(\\phi_1(x))$. By definition:\n",
    "\n",
    "$$K(x;z)=\\phi_2(\\phi_1(x))^T\\phi_2(\\phi_1(z))=K_2(\\phi_1(x);\\phi_1(z))$$\n",
    "\n",
    "Then:\n",
    "    $$K(x;z)=\\phi_2(\\phi_1(x))^T\\phi_2(\\phi_1(z))=K_2(\\phi_1(x);\\phi_1(z))=(\\alpha_2 \\phi_1(x)^T\\phi_1(z)+c_2)^{d_2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition:\n",
    "    $$ \\phi_1(x)^T\\phi_1(z) = K_1(x;z) = (\\alpha_1 x^Tz+c_1)^{d_1}$$\n",
    "    \n",
    "Thus:\n",
    "    $$K(x;z)=(\\alpha_2 (\\alpha_1 x^Tz+c_1)^{d_1}+c_2)^{d_2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the kernel for projection from space of $x$'s to space $\\phi_2(\\phi_1(x))$ is still polinomial kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $K_1,K_2$ be polinomial kernels:\n",
    "$$K_1=exp(-\\epsilon_1(x-z)^2), \\qquad K_2=exp(-\\epsilon_2(x-z)^2)$$\n",
    "\n",
    "Let's consder $K$ that describes projection from $x$ space to $\\phi_2(\\phi_1(x))$. By definition:\n",
    "\n",
    "$$K(x;z)=\\phi_2(\\phi_1(x))^T\\phi_2(\\phi_1(z))=K_2(\\phi_1(x);\\phi_1(z))$$\n",
    "\n",
    "Then:\n",
    "    $$K(x;z)=exp(-\\epsilon_2(\\phi_1(x)-\\phi_1(z))^2)=exp(-\\epsilon_2(\\phi_1(x)-\\phi_1(z))^T(\\phi_1(x)-\\phi_1(z)))=\\\\\n",
    "    =exp\\left(-\\epsilon_2\\left(\\phi_1(x)^T\\phi_1(x)-2\\phi_1(x)^T\\phi_1(z) + \\phi_1(z)^T\\phi_1(z)\\right)\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition:\n",
    "    $$ \\phi_1(x)^T\\phi_1(z) = K_1(x;z)=exp(-\\epsilon_1(x-z)^2)$$\n",
    "    $$ \\phi_1(x)^T\\phi_1(x) = K_1(x;x)=exp(-\\epsilon_1(x-x)^2)=e^0=1$$\n",
    "    $$ \\phi_1(z)^T\\phi_1(z) = K_1(z;z)=exp(-\\epsilon_1(z-z)^2)=e^0=1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus:\n",
    "    $$K(x;z)=exp\\left(-\\epsilon_2\\left(2-2exp(-\\epsilon_1(x-z)^2)\\right)\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the kernel for projection from space of $x$'s to space $\\phi_2(\\phi_1(x))$ is still radial basis function - it depends only on distance from the given point to other considered points. However, this dependency becomes more complex and it is not radial basis kernel (according to its definition)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider the following formula:\n",
    "\n",
    "$$tanh(x)=\\frac{e^x-e^{-x}}{e^x+e^{-x}}=\\frac{1-e^{-2x}}{1+e^{-2x}}= \\frac{2 - (1+e^{-2x})}{1+e^{-2x}}= \\frac{2}{1+e^{-2x}} - 1 = 2sigmoid(2x)-1 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So hyperbolic tangent can be expressed as a linear function of sigmoid and vice versa:\n",
    "$$sig(x)=\\frac{tanh(x/2)+1}{2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the one layer perceptron: ![opl](olp.jpg) (https://wagenaartje.github.io/neataptic/docs/builtins/perceptron/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It has n input neurons, m in the hidden layer, 1 at the output. Let weights of connection between input and hidden layers be $\\omega_{1ij}$, where i is the number of input neuron and j is the number of hidden layer's neron that are connected. Input of j-th hidden layers's neuron - $X_j$, while its output - $y_j$. Output neuron receives $Y=\\sum_{j=1}^{n}\\omega_{2j} y_j$\n",
    "\n",
    "Now let's replace sigmoid activation function with tanh at the hidden layer.\n",
    "\n",
    "Then, we can divide weights between input and hidden layer by 2. Then input of each neuron of the hidden layer will be:\n",
    "$$X'_j=\\sum_{i=1}^{n}(0.5\\omega_{1ij} x_i)=\\frac{X_j}{2}$$\n",
    "\n",
    "Thus it's output: \n",
    "$$y'_j=tanh(X'_j)=2sigmoid(2X'_j)1=2 sigmoid(X_j)-1=2y_j-1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we now divide by 2 all weights between the hidden layer and output neuron, it will receive:\n",
    "$$Y'=\\sum_{j=1}^{n}\\omega'_{2j}y'_j=\\sum_{j=1}^{n}0.5\\omega_{2j} (2y_j-1)=\\sum_{j=1}^{n}(\\omega_{2j}y_j-0.5\\omega_{2j})=Y-\\frac{1}{2}\\sum_{j=1}^{n}\\omega_{2j}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So if we additionally introduce bias equal $\\frac{1}{2}\\sum_{j=1}^{n}\\omega_{2j}$ at the hidden layer (or increase it by the given value if it already exist), the output neuron will receive:\n",
    "$$Y'=Y-\\frac{1}{2}\\sum_{j=1}^{n}\\omega_{2j}+\\frac{1}{2}\\sum_{j=1}^{n}\\omega_{2j}=Y$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence, the received network will compute the same value for all entries in the data, although it uses tanh instead of sigmoid."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However if we introduce the second output neuron (or consider multilayer perceptron) the single bias will not be able to fix this difference between $Y'_j$ and $Y_j$ as it depends on the sum of weights of inputs connected to this neuron. For example for two neurons of the next layer:\n",
    "$$Y'_1=Y_1-\\frac{1}{2}\\sum_{j=1}^{n}\\omega_{2j1}, \\qquad Y'_2=Y_2-\\frac{1}{2}\\sum_{j=1}^{n}\\omega_{2j2},$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can add their own biases for each neuron of the next layer and then generalize our conclusion - we can replace sigmoid with tanh, introduce some changes in the network and receive network that calculates the same value as the old one.\n",
    "But is it legitimate change of neural network?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
