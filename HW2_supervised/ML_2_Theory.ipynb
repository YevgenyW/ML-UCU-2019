{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine Learning\n",
    "<br>Assignment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task  Kernels creation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a) $K(x,z) = K_1(x,z) + K_2(x,z)$\n",
    "<br>According to Mercer theorem, for K to be a valid kernel it is necessary and sufficient that the corresponding kernel matrix is symmetric positive semidefinite matrix.\n",
    "<br> Let's assume that $K, K_1, K_2$ is the appropriate kernel matrix for the kernel function $K(x,z), K_1(x,z), K_2(x,z).$\n",
    "<br> It's known fact that sum of two symmetric matrices is a symmetric matrix.\n",
    "<br> A square symmetric matrix $K \\in R^{m\\times m}$ is positive semidefinite if:\n",
    "$$v^TKv \\ge 0.$$\n",
    "$$v^TKv = v^T(K_1 + K_2)v = v^TK_1v + v^TK_2v \\ge 0.$$\n",
    "So, $K(x,z) = K_1(x,z) + K_2(x,z)$ is a <b>valid kernel</b>.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b) $K(x,z) = K_1(x,z) - K_2(x,z)$\n",
    "<br> Let's assume that kernel function: $K_1(x,z) = \\phi(x)^T\\phi(z) = x^Tz,\\space K_2(x,z) = 2x^Tz.$\n",
    "$$K(x,z) = K_1(x,z) - K_2(x,z) = x^Tz - 2x^Tz = -x^Tz.$$\n",
    "For example, $X =\\begin{bmatrix} x_1\\\\x_2 \\end{bmatrix} = \\begin{bmatrix} 1\\\\2 \\end{bmatrix} \\rightarrow K =\\begin{bmatrix} K(x_1;x_1) & K(x_1;x_2)\\\\K(x_2;x_1) & K(x_2;x_2) \\end{bmatrix}= \\begin{bmatrix} -1 & -2\\\\-2 &-4 \\end{bmatrix}$ - not a positive semidefinite matrix.\n",
    "<br>So, $K(x,z) = K_1(x,z) - K_2(x,z)$ is <b>not a valid kernel</b>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c) $K(x,z) = aK_1(x,z)$\n",
    "<br> It's known fact that multiplication of a symmetric matrix by a scalar is a symmetric matrix.\n",
    "$$v^TKv = v^T(aK_1)v = av^TK_1v \\ge 0.$$\n",
    "So, $K(x,z) = aK_1(x,z)$ is a <b> valid kernel</b>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## d) $K(x,z) = -aK_1(x,z)$\n",
    "$$v^TKv = v^T(-aK_1)v = -av^TK_1v \\le 0 - \\text{not a valid kernel}.$$\n",
    "So, $K(x,z) = -aK_1(x,z)$ is <b>not a valid kernel</b>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## e) $K(x,z) = K_1(ax,bz)$\n",
    "<br>For example, kernel function: $K_1(x,z) = e^{x+z}$. If $X=\\begin{bmatrix}x_1\\\\x_2\\end{bmatrix}$ the following kernel matrix is $K_1 = \\begin{bmatrix}e^{2x_1} & e^{x_1+x_2}\\\\e^{x_1+x_2} & e^{2x_2}\\end{bmatrix}.$\n",
    "<br>If $K_1(ax,bz) = e^{ax+bz}$. If $X=\\begin{bmatrix}x_1\\\\x_2\\end{bmatrix}$ the following kernel matrix is $K_1 = \\begin{bmatrix}e^{(a+b)x_1} & e^{ax_1+bx_2}\\\\e^{ax_2+bx_1} & e^{(a+b)x_2}\\end{bmatrix}$ - not a symmetric matrix.\n",
    "<br>So, $K(x,z) = K_1(ax,bz)$ is <b>not a valid kernel</b>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## f) $K(x,z) = K_1(x,z)\\cdot K_2(x,z)$\n",
    "$$K_1(x,z) = \\phi_1(x)^T\\cdot \\phi_1(z) = \\sum_{i=1}^d{\\phi_1(x)^{(i)}\\cdot \\phi_1(z)^{(i)}}\\text{,}$$\n",
    "$$K_2(x,z) = \\phi_2(x)^T\\cdot \\phi_2(z) = \\sum_{j=1}^d{\\phi_2(x)^{(j)}\\cdot \\phi_2(z)^{(j)}}\\text{,}$$\n",
    "where $d$ - dimension of vector $\\phi$.\n",
    "$$K(x,z) = K_1(x,z)\\cdot K_2(x,z) = \\sum_{i=1}^d{\\sum_{j=1}^d{\\phi_1(x)^{(i)}\\cdot \\phi_1(z)^{(i)}} \\cdot \\phi_2(x)^{(j)}\\cdot \\phi_2(z)^{(j)}} = \\\\ =  \n",
    "\\sum_{i=1}^d{\\sum_{j=1}^d{(\\phi_1(x)^{(i)}\\cdot \\phi_2(x)^{(j)})  \\cdot (\\phi_1(z)^{(i)}} \\cdot \\phi_2(z)^{(j)})} = \\gamma(x)^T \\cdot \\gamma(z).$$\n",
    "<br> So $K(x,z) = K_1(x,z)\\cdot K_2(x,z)$ - <b>valid kernel</b>.\n",
    "<br> It also can be proved with help of Schur product theorem:\n",
    "<br> - elementwise product of two symmetric matrix is still symmetric matrix.\n",
    "<br> - according to Schur product theorem (https://en.wikipedia.org/wiki/Schur_product_theorem) the Hadamard(elementwise) product of two positive definite matrices is also a positive definite matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## g) $K(x,z) = f(x)f(z)$\n",
    "<br> According to definition $f:R^n \\rightarrow R^1$, in this case $f(x)=f(x)^T$. \n",
    "<br>So $K(x,z) = f(x)f(z) = f(x)^Tf(z) = \\phi(x)^T\\phi(z)$ - <b>valid kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## h) $K(x,z) = K_3(\\phi(x),\\space \\phi(z))$\n",
    "<br>$k(x,z) = K_3(\\phi(x),\\space \\phi(z)) = \\psi(\\phi(x))^T\\psi(\\phi(z)) = g(x)^Tg(z)$, where $g(x) = \\psi(\\phi(x)).$\n",
    "<br>So $K(x,z) = K_3(\\phi(x),\\space \\phi(z))$ - <b>valid kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## i) $K(x,z) = p(K_1(x,\\space z))$\n",
    "<br>In general form: $p(x) = a_nx^n + a_{n-1}x^{n-1} + a_1x + a_0. $\n",
    "<br> First, let's prove that $K(x,z) = a_0$ is a kernel function: $K(x,z) = \\phi(x)^T\\phi(z) = a_0, \\space \\phi(x) = \\sqrt{a_0}.$\n",
    "$$p(K_1(x,\\space z)) = p(\\phi(x)^T \\phi(z)) =  a_n(\\phi(x)^T \\phi(z))^{n} + a_{n-1}(\\phi(x)^T \\phi(z))^{n-1} + a_1(\\phi(x)^T \\phi(z)) + a_0. $$\n",
    "<br> As it was proven earlier:\n",
    "$$a_0 - \\text{kernel}$$\n",
    "$$a_n(\\phi(x)^T \\phi(z))^{n}, a_{n-1}(\\phi(x)^T \\phi(z))^{n-1}, a_1(\\phi(x)^T \\phi(z))  - \\text{multiplication of kernels is a kernel}.$$\n",
    "$$ a_n(\\phi(x)^T \\phi(z))^{n} + a_{n-1}(\\phi(x)^T \\phi(z))^{n-1} + a_1(\\phi(x)^T \\phi(z)) + a_0 - \\text{sum of kernels is a kernel}.$$\n",
    "<br>So $K(x,z) = p(K_1(x,\\space z))$ - <b>valid kernel.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## j) $K(x,z) = aK_1(x,\\space z)) - bK_2(x,\\space z))$\n",
    "<br> For example, kernel functions: $K_1(x,\\space z) = xz,\\space K_2(x,\\space z) = 2xz,\\space a=1,\\space b=1.$ \n",
    "<br>In this case:\n",
    "$$K(x,z) = aK_1(x,\\space z)) - bK_2(x,\\space z)) = xz - 2xz = -xz$$\n",
    "<br> So $K(x,z) = aK_1(x,\\space z)) - bK_2(x,\\space z))$ - <b> not a valid kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k) $K(x,z) = -aK_1(x,\\space z)) - bK_2(x,\\space z))$\n",
    "<br> For example, kernel functions: $K_1(x,\\space z) = 2xz,\\space K_2(x,\\space z) = xz,\\space a=1,\\space b=1.$ \n",
    "<br>In this case:\n",
    "$$K(x,z) = -aK_1(x,\\space z)) - bK_2(x,\\space z)) = -2xz - xz = -3xz.$$\n",
    "<br> So $K(x,z) = -aK_1(x,\\space z)) - bK_2(x,\\space z))$ - <b> not a valid kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2\n",
    "Let $K_1$ describes projection from $x$ to $\\phi_1(x)$ and $K_1$ describes projection from $\\phi_1(x)$ to $\\phi_2(x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a.\n",
    "Let $K_1,K_2$ be linear kernels:\n",
    "$$K_1=\\alpha_1 x^Tz+c_1, \\qquad K_2=\\alpha_2 x^Tz+c_2$$\n",
    "\n",
    "Let's consder $K$ that describes projection from $x$ space to $\\phi_2(\\phi_1(x))$. By definition:\n",
    "\n",
    "$$K(x;z)=\\phi_2(\\phi_1(x))^T\\phi_2(\\phi_1(z))=K_2(\\phi_1(x);\\phi_1(z))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then:\n",
    "    $$K(x;z)=\\phi_2(\\phi_1(x))^T\\phi_2(\\phi_1(z))=K_2(\\phi_1(x);\\phi_1(z))=\\alpha_2 \\phi_1(x)^T\\phi_1(z)+c_2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition:\n",
    "    $$ \\phi_1(x)^T\\phi_1(z) = K_1(x;z) =  \\alpha_1 x^Tz+c_1$$\n",
    "    \n",
    "Thus:\n",
    "    $$K(x;z)=\\alpha_2 (\\alpha_1 x^Tz+c_1) +c_2=\\alpha_2\\alpha_1 x^Tz+(\\alpha_2c_1 +c_2)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the kernel for projection from space of $x$'s to space $\\phi_2(\\phi_1(x))$ is still linear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b.\n",
    "Let $K_1,K_2$ be polinomial kernels:\n",
    "$$K_1=(\\alpha_1 x^Tz+c_1)^{d_1}, \\qquad K_2=(\\alpha_2 x^Tz+c_2)^{d_2}$$\n",
    "\n",
    "Let's consder $K$ that describes projection from $x$ space to $\\phi_2(\\phi_1(x))$. By definition:\n",
    "\n",
    "$$K(x;z)=\\phi_2(\\phi_1(x))^T\\phi_2(\\phi_1(z))=K_2(\\phi_1(x);\\phi_1(z))$$\n",
    "\n",
    "Then:\n",
    "    $$K(x;z)=\\phi_2(\\phi_1(x))^T\\phi_2(\\phi_1(z))=K_2(\\phi_1(x);\\phi_1(z))=(\\alpha_2 \\phi_1(x)^T\\phi_1(z)+c_2)^{d_2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition:\n",
    "    $$ \\phi_1(x)^T\\phi_1(z) = K_1(x;z) = (\\alpha_1 x^Tz+c_1)^{d_1}$$\n",
    "    \n",
    "Thus:\n",
    "    $$K(x;z)=(\\alpha_2 (\\alpha_1 x^Tz+c_1)^{d_1}+c_2)^{d_2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the kernel for projection from space of $x$'s to space $\\phi_2(\\phi_1(x))$ is still polinomial kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $K_1,K_2$ be polinomial kernels:\n",
    "$$K_1=exp(-\\epsilon_1(x-z)^2), \\qquad K_2=exp(-\\epsilon_2(x-z)^2)$$\n",
    "\n",
    "Let's consder $K$ that describes projection from $x$ space to $\\phi_2(\\phi_1(x))$. By definition:\n",
    "\n",
    "$$K(x;z)=\\phi_2(\\phi_1(x))^T\\phi_2(\\phi_1(z))=K_2(\\phi_1(x);\\phi_1(z))$$\n",
    "\n",
    "Then:\n",
    "    $$K(x;z)=exp(-\\epsilon_2(\\phi_1(x)-\\phi_1(z))^2)=exp(-\\epsilon_2(\\phi_1(x)-\\phi_1(z))^T(\\phi_1(x)-\\phi_1(z)))=\\\\\n",
    "    =exp\\left(-\\epsilon_2\\left(\\phi_1(x)^T\\phi_1(x)-2\\phi_1(x)^T\\phi_1(z) + \\phi_1(z)^T\\phi_1(z)\\right)\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition:\n",
    "    $$ \\phi_1(x)^T\\phi_1(z) = K_1(x;z)=exp(-\\epsilon_1(x-z)^2)$$\n",
    "    $$ \\phi_1(x)^T\\phi_1(x) = K_1(x;x)=exp(-\\epsilon_1(x-x)^2)=e^0=1$$\n",
    "    $$ \\phi_1(z)^T\\phi_1(z) = K_1(z;z)=exp(-\\epsilon_1(z-z)^2)=e^0=1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus:\n",
    "    $$K(x;z)=exp\\left(-\\epsilon_2\\left(2-2exp(-\\epsilon_1(x-z)^2)\\right)\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the kernel for projection from space of $x$'s to space $\\phi_2(\\phi_1(x))$ is still radial basis function - it depends only on distance from the given point to other considered points. However, this dependency becomes more complex and it is not gaussian radial basis kernel (according to its definition)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider the following formula:\n",
    "\n",
    "$$tanh(x)=\\frac{e^x-e^{-x}}{e^x+e^{-x}}=\\frac{1-e^{-2x}}{1+e^{-2x}}= \\frac{2 - (1+e^{-2x})}{1+e^{-2x}}= \\frac{2}{1+e^{-2x}} - 1 = 2sigmoid(2x)-1 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So hyperbolic tangent can be expressed as a linear function of sigmoid and vice versa:\n",
    "$$sig(x)=\\frac{tanh(x/2)+1}{2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the one layer perceptron: ![opl](olp.jpg) (https://wagenaartje.github.io/neataptic/docs/builtins/perceptron/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It has n input neurons, m in the hidden layer, 1 at the output. Let weights of connection between input and hidden layers be $\\omega_{1ij}$, where i is the number of input neuron and j is the number of hidden layer's neron that are connected. Input of j-th hidden layers's neuron - $X_j$, while its output - $y_j$. Output neuron receives $Y=\\sum_{j=1}^{n}\\omega_{2j} y_j$\n",
    "\n",
    "Now let's replace sigmoid activation function with tanh at the hidden layer.\n",
    "\n",
    "Then, we can divide weights between input and hidden layer by 2. Then input of each neuron of the hidden layer will be:\n",
    "$$X'_j=\\sum_{i=1}^{n}(0.5\\omega_{1ij} x_i)=\\frac{X_j}{2}$$\n",
    "\n",
    "Thus it's output: \n",
    "$$y'_j=tanh(X'_j)=2sigmoid(2X'_j)1=2 sigmoid(X_j)-1=2y_j-1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we now divide by 2 all weights between the hidden layer and output neuron, it will receive:\n",
    "$$Y'=\\sum_{j=1}^{n}\\omega'_{2j}y'_j=\\sum_{j=1}^{n}0.5\\omega_{2j} (2y_j-1)=\\sum_{j=1}^{n}(\\omega_{2j}y_j-0.5\\omega_{2j})=Y-\\frac{1}{2}\\sum_{j=1}^{n}\\omega_{2j}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So if we additionally introduce bias equal $\\frac{1}{2}\\sum_{j=1}^{n}\\omega_{2j}$ at the hidden layer (or increase it by the given value if it already exist), the output neuron will receive:\n",
    "$$Y'=Y-\\frac{1}{2}\\sum_{j=1}^{n}\\omega_{2j}+\\frac{1}{2}\\sum_{j=1}^{n}\\omega_{2j}=Y$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence, the received network will compute the same value for all entries in the data, although it uses tanh instead of sigmoid."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However if we introduce the second output neuron (or consider multilayer perceptron) the single bias will not be able to fix this difference between $Y'_j$ and $Y_j$ as it depends on the sum of weights of inputs connected to this neuron. For example for two neurons of the next layer:\n",
    "$$Y'_1=Y_1-\\frac{1}{2}\\sum_{j=1}^{n}\\omega_{2j1}, \\qquad Y'_2=Y_2-\\frac{1}{2}\\sum_{j=1}^{n}\\omega_{2j2},$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can add their own biases for each neuron of the next layer and then generalize our conclusion - we can replace sigmoid with tanh, introduce some changes in the network and receive network that calculates the same value as the old one.\n",
    "But is it legitimate change of neural network?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
