{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning\n",
    "# Assignment 2. Task 4\n",
    "## Oleh Lukianykhin, Yevhen Pozdniakov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Indoor localization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An indoor positioning system (IPS) is a system to locate objects or people inside a building using radio waves, magnetic fields, acoustic signals, or other sensory information collected by mobile devices. There are several commercial systems on the market, but there is no standard for an IPS system.\n",
    "\n",
    "IPSes use different technologies, including distance measurement to nearby anchor nodes (nodes with known positions, e.g., WiFi access points), magnetic positioning, dead reckoning. They either actively locate mobile devices and tags or provide ambient location or environmental context for devices to get sensed.\n",
    "\n",
    "According to the [report](https://www.marketsandmarkets.com/Market-Reports/indoor-positioning-navigation-ipin-market-989.html), the global indoor location market size is expected to grow from USD 7.11 Billion in 2017 to USD 40.99 Billion by 2022, at a Compound Annual Growth Rate (CAGR) of 42.0% during the forecast period. Hassle-free navigation, improved decision-making, and increased adoption of connected devices are boosting the growth of the indoor location market across the globe.\n",
    "\n",
    "In this problem, you are going to use signals from seven different wi-fi access points to define in which room the user is located."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import numpy as np\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 17\n",
    "np.random.seed(random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the data and breaking it into training and cross-validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = pandas.read_csv('train_set.csv')\n",
    "cv_set = pandas.read_csv('cv_set.csv')\n",
    "\n",
    "train_data = train_set[['wifi'+str(i) for i in range(1, len(train_set.columns) - 1)]]\n",
    "train_labels = train_set['room']\n",
    "cv_data = cv_set[['wifi'+str(i) for i in range(1, len(cv_set.columns) - 1)]]\n",
    "cv_labels = cv_set['room']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   wifi1  wifi2  wifi3  wifi4  wifi5  wifi6  wifi7\n",
      "0    -68    -57    -61    -65    -71    -85    -85\n",
      "1    -63    -60    -60    -67    -76    -85    -84\n",
      "2    -61    -60    -68    -62    -77    -90    -80\n",
      "3    -65    -61    -65    -67    -69    -87    -84\n",
      "4    -61    -63    -58    -66    -74    -87    -82\n",
      "5    -62    -60    -66    -68    -80    -86    -91\n",
      "6    -65    -59    -61    -67    -72    -86    -81\n",
      "7    -63    -57    -61    -65    -73    -84    -84\n",
      "8    -66    -60    -65    -62    -70    -85    -83\n",
      "9    -67    -60    -59    -61    -71    -86    -91\n",
      "0    1\n",
      "1    1\n",
      "2    1\n",
      "3    1\n",
      "4    1\n",
      "5    1\n",
      "6    1\n",
      "7    1\n",
      "8    1\n",
      "9    1\n",
      "Name: room, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(train_data[:10])\n",
    "print(train_labels[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   wifi1  wifi2  wifi3  wifi4  wifi5  wifi6  wifi7\n",
      "0    -64    -56    -61    -66    -71    -82    -81\n",
      "1    -63    -65    -60    -63    -77    -81    -87\n",
      "2    -64    -55    -63    -66    -76    -88    -83\n",
      "3    -65    -60    -59    -63    -76    -86    -82\n",
      "4    -67    -61    -62    -67    -77    -83    -91\n",
      "5    -61    -59    -65    -63    -74    -89    -87\n",
      "6    -63    -56    -63    -65    -72    -82    -89\n",
      "7    -66    -59    -64    -68    -68    -97    -83\n",
      "8    -67    -57    -64    -71    -75    -89    -87\n",
      "9    -63    -57    -59    -67    -71    -82    -93\n",
      "0    1\n",
      "1    1\n",
      "2    1\n",
      "3    1\n",
      "4    1\n",
      "5    1\n",
      "6    1\n",
      "7    1\n",
      "8    1\n",
      "9    1\n",
      "Name: room, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(cv_data[:10])\n",
    "print(cv_labels[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training XGBoost regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First let's try default parameters for gradient boosting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_train = xgb.DMatrix(train_data, train_labels, feature_names=train_data.columns)\n",
    "xgb_test = xgb.DMatrix(cv_data, cv_labels, feature_names=cv_data.columns)\n",
    "parameters = {\n",
    "    \"random_seed\": random_seed,\n",
    "    'num_class':7,\n",
    "    'silent': 1,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 907 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = xgb.train(parameters, xgb_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test error = 0.022670025188916875\n"
     ]
    }
   ],
   "source": [
    "pred = model.predict(xgb_test)\n",
    "\n",
    "error_rate = np.sum(pred != cv_labels) / cv_labels.shape[0]\n",
    "print('Test error = {}'.format(error_rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We received pretty good results - 2.27% of prediction errors.\n",
    "However, we would like to improve prediction quality, thus we will try parameters other than default ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First, we just try other parameters randomly. Learning rate - `eta`, maximum depth of the grown trees - `max_depth`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test error = 0.027707808564231738\n",
      "Wall time: 51.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "parameters = {\n",
    "    'objective': 'multi:softmax',\n",
    "    'eta': 0.1,\n",
    "    \"nthread\": 3,\n",
    "    'silent': 1,\n",
    "    \"random_seed\": 1,\n",
    "    'num_class':7,\n",
    "    'max_depth':5 \n",
    "}\n",
    "model = xgb.train(parameters, xgb_train)\n",
    "pred = model.predict(xgb_test)\n",
    "\n",
    "error_rate = np.sum(pred != cv_labels) / cv_labels.shape[0]\n",
    "print('Test error = {}'.format(error_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test error = 0.015113350125944584\n",
      "Wall time: 42.8 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "parameters = {\n",
    "    'objective': 'multi:softmax',\n",
    "    'eta': 0.5,\n",
    "    \"nthread\": 3,\n",
    "    'silent': 1,\n",
    "    \"random_seed\": 1,\n",
    "    'num_class':7,\n",
    "    'max_depth':3 \n",
    "}\n",
    "model = xgb.train(parameters, xgb_train)\n",
    "pred = model.predict(xgb_test)\n",
    "\n",
    "error_rate = np.sum(pred != cv_labels) / cv_labels.shape[0]\n",
    "print('Test error = {}'.format(error_rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We see that in the first case we received worse result on the test data - 2.77% of errors, while in the second case prediction quality became significantly better - 1.51% of errors.\n",
    "\n",
    "#### What conclusion can we make? Obviously, just trying certain combinations of parameters manually is not a good approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thus we will try to find proper parameters using cross-validation\n",
    "### First, we tried different values of `eta`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eta=0.0, test logloss =1.94591\n",
      "eta=0.01, test logloss =0.6111525999999999\n",
      "eta=0.02, test logloss =0.2655008\n",
      "eta=0.03, test logloss =0.14116900000000002\n",
      "eta=0.04, test logloss =0.0940986\n",
      "eta=0.05, test logloss =0.0763152\n",
      "eta=0.06, test logloss =0.06847120000000001\n",
      "eta=0.07, test logloss =0.06524640000000001\n",
      "eta=0.08, test logloss =0.06503080000000001\n",
      "eta=0.09, test logloss =0.0643112\n",
      "eta=0.1, test logloss =0.0644088\n",
      "eta=0.11, test logloss =0.0641914\n",
      "eta=0.12, test logloss =0.0639374\n",
      "eta=0.13, test logloss =0.063549\n",
      "eta=0.14, test logloss =0.06429119999999999\n",
      "eta=0.15, test logloss =0.0644992\n",
      "eta=0.16, test logloss =0.0639346\n",
      "eta=0.17, test logloss =0.0632468\n",
      "eta=0.18, test logloss =0.0631302\n",
      "eta=0.19, test logloss =0.06440939999999999\n",
      "eta=0.2, test logloss =0.063712\n",
      "eta=0.21, test logloss =0.0640918\n",
      "eta=0.22, test logloss =0.06472460000000001\n",
      "eta=0.23, test logloss =0.064371\n",
      "eta=0.24, test logloss =0.06482180000000001\n",
      "eta=0.25, test logloss =0.06462380000000001\n",
      "eta=0.26, test logloss =0.06282360000000001\n",
      "eta=0.27, test logloss =0.063358\n",
      "eta=0.28, test logloss =0.0637966\n",
      "eta=0.29, test logloss =0.0646186\n",
      "eta=0.3, test logloss =0.064668\n",
      "eta=0.31, test logloss =0.0628652\n",
      "eta=0.32, test logloss =0.06200880000000001\n",
      "eta=0.33, test logloss =0.062997\n",
      "eta=0.34, test logloss =0.0636204\n",
      "eta=0.35000000000000003, test logloss =0.0602422\n",
      "eta=0.36, test logloss =0.0636476\n",
      "eta=0.37, test logloss =0.0648276\n",
      "eta=0.38, test logloss =0.06316560000000002\n",
      "eta=0.39, test logloss =0.0633964\n",
      "eta=0.4, test logloss =0.06496\n",
      "eta=0.41000000000000003, test logloss =0.06438980000000001\n",
      "eta=0.42, test logloss =0.0627654\n",
      "eta=0.43, test logloss =0.062624\n",
      "eta=0.44, test logloss =0.0637468\n",
      "eta=0.45, test logloss =0.0622814\n",
      "eta=0.46, test logloss =0.0652772\n",
      "eta=0.47000000000000003, test logloss =0.0640454\n",
      "eta=0.48, test logloss =0.0639374\n",
      "eta=0.49, test logloss =0.06491440000000001\n",
      "eta=0.5, test logloss =0.06372320000000001\n",
      "eta=0.51, test logloss =0.06457120000000001\n",
      "eta=0.52, test logloss =0.06525700000000001\n",
      "eta=0.53, test logloss =0.0655716\n",
      "eta=0.54, test logloss =0.064003\n",
      "eta=0.55, test logloss =0.06377580000000001\n",
      "eta=0.56, test logloss =0.06351699999999999\n",
      "eta=0.5700000000000001, test logloss =0.063467\n",
      "eta=0.58, test logloss =0.0659874\n",
      "eta=0.59, test logloss =0.0639858\n",
      "eta=0.6, test logloss =0.0652448\n",
      "eta=0.61, test logloss =0.062007599999999996\n",
      "eta=0.62, test logloss =0.061946000000000015\n",
      "eta=0.63, test logloss =0.06323619999999999\n",
      "eta=0.64, test logloss =0.0633426\n",
      "eta=0.65, test logloss =0.0629952\n",
      "eta=0.66, test logloss =0.06702280000000001\n",
      "eta=0.67, test logloss =0.0651402\n",
      "eta=0.68, test logloss =0.06415119999999999\n",
      "eta=0.6900000000000001, test logloss =0.0629296\n",
      "eta=0.7000000000000001, test logloss =0.0634602\n",
      "eta=0.71, test logloss =0.0650434\n",
      "eta=0.72, test logloss =0.0653304\n",
      "eta=0.73, test logloss =0.0652748\n",
      "eta=0.74, test logloss =0.0646192\n",
      "eta=0.75, test logloss =0.065932\n",
      "eta=0.76, test logloss =0.065241\n",
      "eta=0.77, test logloss =0.067029\n",
      "eta=0.78, test logloss =0.06305540000000001\n",
      "eta=0.79, test logloss =0.0637056\n",
      "eta=0.8, test logloss =0.06581119999999999\n",
      "eta=0.81, test logloss =0.0660246\n",
      "eta=0.8200000000000001, test logloss =0.06302560000000001\n",
      "eta=0.8300000000000001, test logloss =0.06527440000000001\n",
      "eta=0.84, test logloss =0.06598319999999999\n",
      "eta=0.85, test logloss =0.0680938\n",
      "eta=0.86, test logloss =0.06463319999999999\n",
      "eta=0.87, test logloss =0.0651818\n",
      "eta=0.88, test logloss =0.0645542\n",
      "eta=0.89, test logloss =0.06476779999999999\n",
      "eta=0.9, test logloss =0.0642798\n",
      "eta=0.91, test logloss =0.0688592\n",
      "eta=0.92, test logloss =0.0660278\n",
      "eta=0.93, test logloss =0.06616280000000001\n",
      "eta=0.9400000000000001, test logloss =0.0692336\n",
      "eta=0.9500000000000001, test logloss =0.0669012\n",
      "eta=0.96, test logloss =0.06856519999999999\n",
      "eta=0.97, test logloss =0.0676258\n",
      "eta=0.98, test logloss =0.0674952\n",
      "eta=0.99, test logloss =0.0633308\n",
      "Wall time: 1min 27s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "for eta in np.arange(0,1,0.01):\n",
    "    parameters = {\n",
    "        #default\n",
    "        'objective': 'multi:softmax',\n",
    "        'silent': 1,\n",
    "        \"nthread\": 3,\n",
    "        \"random_seed\": 1,\n",
    "        \"eval_metric\": 'mlogloss',\n",
    "        'num_class':7,\n",
    "        'eta':eta\n",
    "    }\n",
    "    results = xgb.cv(parameters, xgb_train, num_boost_round = 100, early_stopping_rounds=10,\n",
    "                     nfold=5, stratified=True, seed=random_seed, show_stdv=False, \n",
    "                     verbose_eval=False)\n",
    "    print(\"eta={}, test logloss ={}\".format(eta, results.iloc[-1]['test-mlogloss-mean']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We have tried several local minimums and decided to take `eta=0.62` for further consideration, corresponding mean mlogloss on the test data equals ~0.0619"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test error = 0.015113350125944584\n"
     ]
    }
   ],
   "source": [
    "parameters = {\n",
    "   'objective': 'multi:softmax',\n",
    "    'eta': 0.62,\n",
    "    'silent': 1,\n",
    "    \"nthread\": 3,\n",
    "    \"random_seed\": 1,\n",
    "    'num_class':7,\n",
    "    \n",
    "}\n",
    "model = xgb.train(parameters, xgb_train)\n",
    "pred = model.predict(xgb_test)\n",
    "\n",
    "error_rate = np.sum(pred != cv_labels) / cv_labels.shape[0]\n",
    "print('Test error = {}'.format(error_rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corresponding error on the test data - 1.51%\n",
    "### Next, we decided to try different values of `max_depth`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_depth=1, test logloss =0.055717\n",
      "max_depth=2, test logloss =0.057705799999999995\n",
      "max_depth=3, test logloss =0.059850799999999996\n",
      "max_depth=4, test logloss =0.0635156\n",
      "max_depth=5, test logloss =0.06320980000000001\n",
      "max_depth=6, test logloss =0.061946000000000015\n",
      "max_depth=7, test logloss =0.06206339999999999\n",
      "max_depth=8, test logloss =0.06233780000000001\n",
      "max_depth=9, test logloss =0.062733\n",
      "Wall time: 6.06 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "for max_depth in np.arange(1,10):\n",
    "    parameters = {\n",
    "        #default\n",
    "        'objective': 'multi:softmax',\n",
    "        'silent': 1,\n",
    "        \"nthread\": 3,\n",
    "        \"random_seed\": 1,\n",
    "        \"eval_metric\": 'mlogloss',\n",
    "        'num_class':7,\n",
    "        'eta': 0.62,\n",
    "        'max_depth':max_depth\n",
    "    }\n",
    "    results = xgb.cv(parameters, xgb_train, num_boost_round = 100, early_stopping_rounds=10,\n",
    "                     nfold=5, stratified=True, seed=random_seed, show_stdv=False, \n",
    "                     verbose_eval=False)\n",
    "    print(\"max_depth={}, test logloss ={}\".format(max_depth, results.iloc[-1]['test-mlogloss-mean']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We have tried several values and decided to use `max_depth=3`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test error = 0.012594458438287154\n"
     ]
    }
   ],
   "source": [
    "parameters = {\n",
    "   'objective': 'multi:softmax',\n",
    "    'eta': 0.62,\n",
    "    'silent': 1,\n",
    "    \"nthread\": 3,\n",
    "    \"random_seed\": 1,\n",
    "    'num_class':7,\n",
    "    'max_depth':3\n",
    "}\n",
    "model = xgb.train(parameters, xgb_train)\n",
    "pred = model.predict(xgb_test)\n",
    "\n",
    "error_rate = np.sum(pred != cv_labels) / cv_labels.shape[0]\n",
    "print('Test error = {}'.format(error_rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corresponding error on the test data ~1.26%\n",
    "### Next, we decided to try different values of `lambda` - coefficient for L2-regularization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambda=0.0, test logloss =0.0604512\n",
      "lambda=0.05, test logloss =0.05868359999999999\n",
      "lambda=0.1, test logloss =0.057423800000000004\n",
      "lambda=0.15000000000000002, test logloss =0.060823800000000004\n",
      "lambda=0.2, test logloss =0.0599474\n",
      "lambda=0.25, test logloss =0.058122799999999995\n",
      "lambda=0.30000000000000004, test logloss =0.0585228\n",
      "lambda=0.35000000000000003, test logloss =0.057076800000000004\n",
      "lambda=0.4, test logloss =0.056756400000000005\n",
      "lambda=0.45, test logloss =0.059345999999999996\n",
      "lambda=0.5, test logloss =0.059927400000000006\n",
      "lambda=0.55, test logloss =0.0580032\n",
      "lambda=0.6000000000000001, test logloss =0.057970999999999995\n",
      "lambda=0.65, test logloss =0.0568608\n",
      "lambda=0.7000000000000001, test logloss =0.060494400000000004\n",
      "lambda=0.75, test logloss =0.05949800000000001\n",
      "lambda=0.8, test logloss =0.060073999999999995\n",
      "lambda=0.8500000000000001, test logloss =0.059630800000000005\n",
      "lambda=0.9, test logloss =0.061152200000000004\n",
      "lambda=0.9500000000000001, test logloss =0.060014\n",
      "lambda=1.0, test logloss =0.059850799999999996\n",
      "lambda=1.05, test logloss =0.0590156\n",
      "lambda=1.1, test logloss =0.0601464\n",
      "lambda=1.1500000000000001, test logloss =0.059844400000000006\n",
      "lambda=1.2000000000000002, test logloss =0.0592742\n",
      "lambda=1.25, test logloss =0.059946\n",
      "lambda=1.3, test logloss =0.060174799999999994\n",
      "lambda=1.35, test logloss =0.05892099999999999\n",
      "lambda=1.4000000000000001, test logloss =0.0606216\n",
      "lambda=1.4500000000000002, test logloss =0.06205959999999999\n",
      "lambda=1.5, test logloss =0.060578\n",
      "lambda=1.55, test logloss =0.0610226\n",
      "lambda=1.6, test logloss =0.061893\n",
      "lambda=1.6500000000000001, test logloss =0.059753999999999995\n",
      "lambda=1.7000000000000002, test logloss =0.0593772\n",
      "lambda=1.75, test logloss =0.0623856\n",
      "lambda=1.8, test logloss =0.06060019999999999\n",
      "lambda=1.85, test logloss =0.059922600000000006\n",
      "lambda=1.9000000000000001, test logloss =0.06185099999999999\n",
      "lambda=1.9500000000000002, test logloss =0.06082800000000001\n",
      "lambda=2.0, test logloss =0.060024999999999995\n",
      "lambda=2.0500000000000003, test logloss =0.061232999999999996\n",
      "lambda=2.1, test logloss =0.06072580000000001\n",
      "lambda=2.15, test logloss =0.060603000000000004\n",
      "lambda=2.2, test logloss =0.061034599999999994\n",
      "lambda=2.25, test logloss =0.059565\n",
      "lambda=2.3000000000000003, test logloss =0.05985119999999999\n",
      "lambda=2.35, test logloss =0.060271399999999996\n",
      "lambda=2.4000000000000004, test logloss =0.05894119999999999\n",
      "lambda=2.45, test logloss =0.060605599999999996\n",
      "lambda=2.5, test logloss =0.059414800000000004\n",
      "lambda=2.5500000000000003, test logloss =0.060122999999999996\n",
      "lambda=2.6, test logloss =0.059483600000000005\n",
      "lambda=2.6500000000000004, test logloss =0.060379800000000004\n",
      "lambda=2.7, test logloss =0.06009199999999999\n",
      "lambda=2.75, test logloss =0.05976239999999999\n",
      "lambda=2.8000000000000003, test logloss =0.06049059999999999\n",
      "lambda=2.85, test logloss =0.0594784\n",
      "lambda=2.9000000000000004, test logloss =0.0593668\n",
      "lambda=2.95, test logloss =0.060036400000000004\n",
      "Wall time: 36.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "for lambd in np.arange(0,3,0.05):\n",
    "    parameters = {\n",
    "        #default\n",
    "        'objective': 'multi:softmax',\n",
    "        'silent': 1,\n",
    "        \"nthread\": 3,\n",
    "        \"random_seed\": 1,\n",
    "        \"eval_metric\": 'mlogloss',\n",
    "        'num_class':7,\n",
    "        'eta': 0.62,\n",
    "        'max_depth':3,\n",
    "        'lambda':lambd\n",
    "        \n",
    "    }\n",
    "    results = xgb.cv(parameters, xgb_train, num_boost_round = 100, early_stopping_rounds=10,\n",
    "                     nfold=5, stratified=True, seed=random_seed, show_stdv=False, \n",
    "                     verbose_eval=False)\n",
    "    \n",
    "    print(\"lambda={}, test logloss ={}\".format(lambd, results.iloc[-1]['test-mlogloss-mean']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We have tried several values for our test data and decided to use `lambda=2`, corresponding mean mlogloss on the test data equals ~0.06002."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test error = 0.010075566750629723\n"
     ]
    }
   ],
   "source": [
    "parameters = {\n",
    "   'objective': 'multi:softmax',\n",
    "    'eta': 0.62,\n",
    "    'silent': 1,\n",
    "    \"nthread\": 3,\n",
    "    \"random_seed\": 1,\n",
    "    'num_class':7,\n",
    "    'max_depth':3,\n",
    "    'lambda': 2\n",
    "}\n",
    "model = xgb.train(parameters, xgb_train)\n",
    "pred = model.predict(xgb_test)\n",
    "\n",
    "error_rate = np.sum(pred != cv_labels) / cv_labels.shape[0]\n",
    "print('Test error = {}'.format(error_rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corresponding error on the test data ~1.01% Much better than our initial guess - 2.27%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## However, I should say that the final decision was made to minimize error on the test data.\n",
    "## This is not the best approach, when we want to solve the real world problem or competition leaderboard is private. In this cases we should rely on cross validation results, not the public leaderboard or one test dataset.\n",
    "\n",
    "## There is more generalised approach to choose the model hyperparameters using crossvalidation - we iterate over all combinations of parameters that we want to consider and choose the best one according to the cross validation results.\n",
    "\n",
    "## This is kind of brute force and can not be easily applied to all tasks. Thus one should:\n",
    "### a) Decrease number of parameter values being considerd. For instance, knowledge about subject field or model properties can be used for this purpose.\n",
    "### b) Try random search - instead of iterating over all combinations we randomly sample one of them from the corresponding space at each step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## So, we decided to try random search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "        'min_child_weight': [1, 5, 10],\n",
    "        'gamma': [0.5, 1, 1.5, 2, 5],\n",
    "        'subsample': [0.6, 0.8, 1.0],\n",
    "        'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "        'max_depth': [1, 2, 3, 4, 5],\n",
    "        'eta':[0.35, 0.62, 0.7],\n",
    "        'n_estimators':[20, 50, 100, 200, 600],\n",
    "        'lambda':[0,0.5,1.5,2,5],\n",
    "        'alpha ':[0,0.5,1.5,2,5]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = XGBClassifier(objective='multi:softmax',\n",
    "                    silent=True, nthread=1, random_seed=random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = 3\n",
    "param_comb = 1000\n",
    "\n",
    "skf = StratifiedKFold(n_splits=folds, shuffle = True, random_state = 1001)\n",
    "\n",
    "random_search = RandomizedSearchCV(xgb, param_distributions=params, n_iter=param_comb, \n",
    "                                    n_jobs=4, cv=skf.split(train_data,train_labels),\n",
    "                                   verbose=3, random_state=random_seed )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1000 candidates, totalling 3000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  24 tasks      | elapsed:   10.3s\n",
      "[Parallel(n_jobs=4)]: Done 120 tasks      | elapsed:   29.8s\n",
      "[Parallel(n_jobs=4)]: Done 280 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=4)]: Done 504 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=4)]: Done 792 tasks      | elapsed:  2.9min\n",
      "[Parallel(n_jobs=4)]: Done 1144 tasks      | elapsed:  4.1min\n",
      "[Parallel(n_jobs=4)]: Done 1560 tasks      | elapsed:  5.8min\n",
      "[Parallel(n_jobs=4)]: Done 2040 tasks      | elapsed:  7.8min\n",
      "[Parallel(n_jobs=4)]: Done 2584 tasks      | elapsed: 10.1min\n",
      "[Parallel(n_jobs=4)]: Done 3000 out of 3000 | elapsed: 11.8min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 11min 51s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=<generator object _BaseKFold.split at 0x000002573A03BC50>,\n",
       "          error_score='raise-deprecating',\n",
       "          estimator=XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
       "       max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
       "       n_jobs=1, nthread=1, objective='multi:softmax', random_seed=17,\n",
       "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
       "       seed=None, silent=True, subsample=1),\n",
       "          fit_params=None, iid='warn', n_iter=1000, n_jobs=4,\n",
       "          param_distributions={'min_child_weight': [1, 5, 10], 'gamma': [0.5, 1, 1.5, 2, 5], 'subsample': [0.6, 0.8, 1.0], 'colsample_bytree': [0.6, 0.8, 1.0], 'max_depth': [1, 2, 3, 4, 5], 'eta': [0.35, 0.62, 0.7], 'n_estimators': [20, 50, 100, 200, 600], 'lambda': [0, 0.5, 1.5, 2, 5], 'alpha ': [0, 0.5, 1.5, 2, 5]},\n",
       "          pre_dispatch='2*n_jobs', random_state=17, refit=True,\n",
       "          return_train_score='warn', scoring=None, verbose=3)"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "random_search.fit(train_data,train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test error = 0.017632241813602016\n"
     ]
    }
   ],
   "source": [
    "pred = random_search.predict(cv_data)\n",
    "error_rate = np.sum(pred != cv_labels) / cv_labels.shape[0]\n",
    "print('Test error = {}'.format(error_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test error = 0.022670025188916875\n"
     ]
    }
   ],
   "source": [
    "pred = random_search.predict(cv_data)\n",
    "error_rate = np.sum(pred != cv_labels) / cv_labels.shape[0]\n",
    "print('Test error = {}'.format(error_rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unfortuntely, even with a lot of time spent, results on the test data are much worse than our previous options.\n",
    "\n",
    "### Most likely, because we have made too few iterations (to small number of samples from the parameters space) and thus have not found better values for hyperparameters. However, it also may be because high accuracy in the previous case is caused by overfitting."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
