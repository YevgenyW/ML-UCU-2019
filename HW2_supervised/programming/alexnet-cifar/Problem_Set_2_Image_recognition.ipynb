{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wO_b28mMG6u6"
   },
   "source": [
    "# Convolutional neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MLMWHsiJG6u8"
   },
   "source": [
    "In class, we have briefly reviewed the idea of learning good features directly from data and went through the concept of convolutional neural networks along with few architectures.\n",
    "\n",
    "Until recently, building convolutional neural networks was tough. There was no high-level tools for that, you would be required to understand all the internal mechanics of the model and its operations.\n",
    "\n",
    "Today, due to the high-level tools such as Keras and TensorFlow, everybody can build a convolutional neural network and put it to work without diving deep into them. What used to be a one-month project became a few hours exercise.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-07T06:52:06.909011Z",
     "start_time": "2019-03-07T06:52:06.904061Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "luMFM5vbG6u-"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "import keras.utils\n",
    "from keras import utils as np_utils\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.datasets import cifar10\n",
    "from keras import regularizers\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 56
    },
    "colab_type": "code",
    "id": "r7tizbEiYYXL",
    "outputId": "83f6029c-9376-4826-bf9d-a09f7111c57b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/device:GPU:0'"
      ]
     },
     "execution_count": 2,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !pip install fastai\n",
    "tf.test.gpu_device_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-C_2bxDNH3Dr"
   },
   "outputs": [],
   "source": [
    "# !pip install -U -q PyDrive\n",
    "# from pydrive.auth import GoogleAuth\n",
    "# from pydrive.drive import GoogleDrive\n",
    "# from google.colab import auth\n",
    "# from oauth2client.client import GoogleCredentials\n",
    "# # Authenticate and create the PyDrive client.\n",
    "# auth.authenticate_user()\n",
    "# gauth = GoogleAuth()\n",
    "# gauth.credentials = GoogleCredentials.get_application_default()\n",
    "# drive = GoogleDrive(gauth)\n",
    "\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/gdrive', force_remount=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qZ38OA6WG6vE"
   },
   "source": [
    "## Loading the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-07T06:52:08.224401Z",
     "start_time": "2019-03-07T06:52:07.895734Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1020
    },
    "colab_type": "code",
    "id": "ao5YC5toG6vG",
    "outputId": "ce278fc9-62fe-40a4-c1a7-eb8be891a394"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "170500096/170498071 [==============================] - 495s 3us/step\n"
     ]
    }
   ],
   "source": [
    "(train_images, train_labels), (cv_images, cv_labels) = cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-07T06:52:08.346901Z",
     "start_time": "2019-03-07T06:52:08.343638Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "id": "s19rswZfG6vX",
    "outputId": "e16ab50c-fe87-4c53-9289-6155a05ab9b5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 32, 32, 3)\n",
      "50000\n"
     ]
    }
   ],
   "source": [
    "print(train_images.shape)\n",
    "print(len(train_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-07T06:52:09.058269Z",
     "start_time": "2019-03-07T06:52:09.054837Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "id": "2DWLHibvG6vd",
    "outputId": "0a27badb-157b-449f-f9fc-c800e1b18dc1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 32, 32, 3)\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "print(cv_images.shape)\n",
    "print(len(cv_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-07T06:52:09.676138Z",
     "start_time": "2019-03-07T06:52:09.671572Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 213
    },
    "colab_type": "code",
    "id": "Kgjm3RZTG6vj",
    "outputId": "f9eac9a4-7e07-48e4-fc71-046a570963a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6]\n",
      " [9]\n",
      " [9]\n",
      " [4]\n",
      " [1]\n",
      " [1]\n",
      " [2]\n",
      " [7]\n",
      " [8]\n",
      " [3]]\n"
     ]
    }
   ],
   "source": [
    "print(train_labels[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-07T06:52:10.541959Z",
     "start_time": "2019-03-07T06:52:10.534920Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "SyJesOBXG6vo"
   },
   "outputs": [],
   "source": [
    "def print_model_results(model):\n",
    "    score = model.evaluate(cv_images, Y_cv, batch_size=32, verbose=1)\n",
    "    print('Test loss:', score[0])\n",
    "    print('Test accuracy:', score[1])\n",
    "    print(model.metrics_names)\n",
    "    print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-07T06:52:11.349502Z",
     "start_time": "2019-03-07T06:52:11.346229Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "Znq9ePTwG6vq"
   },
   "outputs": [],
   "source": [
    "def show_image(np_array):\n",
    "    %matplotlib inline\n",
    "    plt.figure()\n",
    "    plt.imshow(np_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-07T06:52:11.939606Z",
     "start_time": "2019-03-07T06:52:11.936249Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "1JziPCwkG6vv"
   },
   "outputs": [],
   "source": [
    "def show_example(data_set, labels, example_index):\n",
    "    show_image(data_set[example_index])\n",
    "    print('Label: ', labels[example_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-07T06:52:16.458608Z",
     "start_time": "2019-03-07T06:52:16.304225Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 284
    },
    "colab_type": "code",
    "id": "cUUwRsO8G6vz",
    "outputId": "93da30f5-49a2-459b-df28-eb4984d84308"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label:  [6]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPoAAAD5CAYAAAAOeCiTAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_example(train_images, train_labels, example_index = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-07T06:52:19.119822Z",
     "start_time": "2019-03-07T06:52:18.969167Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 284
    },
    "colab_type": "code",
    "id": "KlP4Ip0KG6wD",
    "outputId": "b399e777-7319-4f38-8b6c-ede06229c0dc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label:  [3]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPoAAAD5CAYAAAAOeCiTAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nuqLEADV0RYkB/w8OoEb2ZrT8nQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_example(cv_images, cv_labels, example_index = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gtnczGOfG6wH"
   },
   "source": [
    "## Building AlexNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KifH7OdOG6wI"
   },
   "source": [
    "In 2012 a convolutional neural network called AlexNet won ImageNet competition. \n",
    "\n",
    "Go through an [original AlexNet paper](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf) to investigate the architecture. Next, investigate the [basics of Keras](https://keras.io/#keras-the-python-deep-learning-library). We will use it with TensorFlow backend.\n",
    "\n",
    "You are also encouraged to go through some CNN tutorial for Keras. There is a number of them online (for example, [this](https://elitedatascience.com/keras-tutorial-deep-learning-in-python) or [this](https://machinelearningmastery.com/handwritten-digit-recognition-using-convolutional-neural-networks-python-keras/)).\n",
    "Now, build AlexNex network with Keras for object recognition. Note that standard AlexNet works with 224x224 input images. The dataset you are going to use for this problem is 32x32."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "itL5AS61G6wJ"
   },
   "source": [
    "## Training AlexNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F3zC4l0NG6wK"
   },
   "source": [
    "Use training set for training the network to recognize objects. You might want to use RMSProp optimizer to speed up the training.\n",
    "\n",
    "Convolutional networks require a lot of computing power for training. Typical setup for training CNN is to use GPU, however, in this problem you are not required to do so. CPU will be fine as well.\n",
    "\n",
    "If you are using CPU for this subproblem, training process might be slow. You can stop it manually as soon as you get meaningful results.\n",
    "\n",
    "Report the results on the training and cross-validation sets. The report should contain the training logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-07T06:53:24.234434Z",
     "start_time": "2019-03-07T06:53:23.653801Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "id": "uphPv82VG6wL",
    "outputId": "9115b074-96b0-4b78-cf21-74b5ce7e3ed8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 3, 32, 32)\n",
      "(50000, 10)\n"
     ]
    }
   ],
   "source": [
    "# preprocessing input data\n",
    "\n",
    "train_images = train_images.reshape(train_images.shape[0], 3, 32, 32)\n",
    "cv_images = cv_images.reshape(cv_images.shape[0], 3, 32, 32)\n",
    "print(train_images.shape)\n",
    "\n",
    "train_images = train_images.astype('float32')\n",
    "cv_images = cv_images.astype('float32')\n",
    "train_images /= 255\n",
    "cv_images /= 255\n",
    "\n",
    "# Convert 1-dimensional class arrays to 10-dimensional class matrices\n",
    "Y_train = np_utils.to_categorical(train_labels, 10)\n",
    "Y_cv = np_utils.to_categorical(cv_labels, 10)\n",
    "print (Y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-07T06:54:02.900192Z",
     "start_time": "2019-03-07T06:54:02.896691Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "pDCgEGPtG6wX"
   },
   "outputs": [],
   "source": [
    "epochs = 50\n",
    "num_classes = 10\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-07T07:05:19.749897Z",
     "start_time": "2019-03-07T07:05:19.735777Z"
    },
    "colab": {},
    "colab_type": "code",
    "hide_input": false,
    "id": "WQw5IpDFG6wa"
   },
   "outputs": [],
   "source": [
    "# https://github.com/jkh911208/cswithjames/blob/master/8_CIFAR10_alexnet.py\n",
    "def create_alexnet_model(need_to_train = False):\n",
    "    # AlexNet Define the Model\n",
    "    model = Sequential()\n",
    "    # model.add(Conv2D(96, (11,11), strides=(4,4), activation='relu', padding='same', input_shape=(img_height, img_width, channel,)))\n",
    "    # for original Alexnet\n",
    "    model.add(Conv2D(96, (3,3), strides=(2,2), activation='relu', \n",
    "                     padding='same', \n",
    "                     input_shape=train_images.shape[1:], \n",
    "                     data_format = 'channels_first'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2,2)))\n",
    "    # Local Response normalization for Original Alexnet\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Conv2D(256, (5,5), activation='relu', padding='same'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(3, 3), strides=(2,2)))\n",
    "    # Local Response normalization for Original Alexnet\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Conv2D(384, (3,3), activation='relu', padding='same'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv2D(384, (3,3), activation='relu', padding='same'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv2D(256, (3,3), activation='relu', padding='same'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(3, 3), strides=(2,2)))\n",
    "    # Local Response normalization for Original Alexnet\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(4096, activation='tanh'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(4096, activation='tanh'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    # print the model summary\n",
    "    model.summary()\n",
    "\n",
    "    # determine Loss function and Optimizer\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    if need_to_train == True:\n",
    "        model.fit(train_images, Y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(cv_images, Y_cv),\n",
    "                    shuffle=True)\n",
    "#         save_model(model_name = 'alexnet_model', model = model)\n",
    "    else:\n",
    "        model = load_model('alexnet_model_not working', model)\n",
    "    \n",
    "    return model\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-07T07:06:35.637705Z",
     "start_time": "2019-03-07T07:05:23.994007Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 2941
    },
    "colab_type": "code",
    "id": "qXTzOQmJG6wb",
    "outputId": "29c89589-abe1-4460-d727-e8a015e7176c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 96, 16, 16)        2688      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 96, 16, 16)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 48, 8, 16)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 48, 8, 16)         64        \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 48, 8, 256)        102656    \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 48, 8, 256)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 23, 3, 256)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 23, 3, 256)        1024      \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 23, 3, 384)        885120    \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 23, 3, 384)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 23, 3, 384)        1327488   \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 23, 3, 384)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 23, 3, 256)        884992    \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 23, 3, 256)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 11, 1, 256)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 11, 1, 256)        1024      \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 2816)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 4096)              11538432  \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                40970     \n",
      "=================================================================\n",
      "Total params: 31,565,770\n",
      "Trainable params: 31,564,714\n",
      "Non-trainable params: 1,056\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "50000/50000 [==============================] - 62s 1ms/step - loss: 2.1978 - acc: 0.2073 - val_loss: 1.8565 - val_acc: 0.2943\n",
      "Epoch 2/50\n",
      "50000/50000 [==============================] - 55s 1ms/step - loss: 1.8765 - acc: 0.2882 - val_loss: 1.8111 - val_acc: 0.3095\n",
      "Epoch 3/50\n",
      "50000/50000 [==============================] - 55s 1ms/step - loss: 1.7817 - acc: 0.3274 - val_loss: 1.7071 - val_acc: 0.3738\n",
      "Epoch 4/50\n",
      "50000/50000 [==============================] - 55s 1ms/step - loss: 1.7386 - acc: 0.3456 - val_loss: 1.6316 - val_acc: 0.3829\n",
      "Epoch 5/50\n",
      "50000/50000 [==============================] - 55s 1ms/step - loss: 1.6999 - acc: 0.3627 - val_loss: 1.7239 - val_acc: 0.3617\n",
      "Epoch 6/50\n",
      "50000/50000 [==============================] - 55s 1ms/step - loss: 1.6783 - acc: 0.3752 - val_loss: 1.6623 - val_acc: 0.3941\n",
      "Epoch 7/50\n",
      "50000/50000 [==============================] - 55s 1ms/step - loss: 1.6577 - acc: 0.3822 - val_loss: 1.6611 - val_acc: 0.3927\n",
      "Epoch 8/50\n",
      "50000/50000 [==============================] - 55s 1ms/step - loss: 1.6416 - acc: 0.3912 - val_loss: 1.6469 - val_acc: 0.4143\n",
      "Epoch 9/50\n",
      "50000/50000 [==============================] - 55s 1ms/step - loss: 1.6243 - acc: 0.4036 - val_loss: 1.6708 - val_acc: 0.3930\n",
      "Epoch 10/50\n",
      "50000/50000 [==============================] - 55s 1ms/step - loss: 1.6034 - acc: 0.4071 - val_loss: 1.6584 - val_acc: 0.4057\n",
      "Epoch 11/50\n",
      "50000/50000 [==============================] - 55s 1ms/step - loss: 1.6058 - acc: 0.4087 - val_loss: 1.5490 - val_acc: 0.4281\n",
      "Epoch 12/50\n",
      "50000/50000 [==============================] - 55s 1ms/step - loss: 1.5924 - acc: 0.4150 - val_loss: 1.6437 - val_acc: 0.3928\n",
      "Epoch 13/50\n",
      "50000/50000 [==============================] - 56s 1ms/step - loss: 1.5818 - acc: 0.4154 - val_loss: 1.5347 - val_acc: 0.4332\n",
      "Epoch 14/50\n",
      "50000/50000 [==============================] - 55s 1ms/step - loss: 1.5603 - acc: 0.4240 - val_loss: 1.5010 - val_acc: 0.4499\n",
      "Epoch 15/50\n",
      "50000/50000 [==============================] - 55s 1ms/step - loss: 1.5637 - acc: 0.4264 - val_loss: 1.5770 - val_acc: 0.4186\n",
      "Epoch 16/50\n",
      "50000/50000 [==============================] - 55s 1ms/step - loss: 1.5530 - acc: 0.4282 - val_loss: 1.4964 - val_acc: 0.4424\n",
      "Epoch 17/50\n",
      "50000/50000 [==============================] - 55s 1ms/step - loss: 1.5483 - acc: 0.4313 - val_loss: 1.6128 - val_acc: 0.4071\n",
      "Epoch 18/50\n",
      "50000/50000 [==============================] - 55s 1ms/step - loss: 1.5393 - acc: 0.4368 - val_loss: 1.5327 - val_acc: 0.4458\n",
      "Epoch 19/50\n",
      "50000/50000 [==============================] - 55s 1ms/step - loss: 1.5254 - acc: 0.4393 - val_loss: 1.5551 - val_acc: 0.4261\n",
      "Epoch 20/50\n",
      "50000/50000 [==============================] - 55s 1ms/step - loss: 1.5222 - acc: 0.4410 - val_loss: 1.5674 - val_acc: 0.4298\n",
      "Epoch 21/50\n",
      "50000/50000 [==============================] - 55s 1ms/step - loss: 1.5261 - acc: 0.4430 - val_loss: 1.5243 - val_acc: 0.4448\n",
      "Epoch 22/50\n",
      "50000/50000 [==============================] - 55s 1ms/step - loss: 1.5053 - acc: 0.4495 - val_loss: 1.5253 - val_acc: 0.4515\n",
      "Epoch 23/50\n",
      "50000/50000 [==============================] - 55s 1ms/step - loss: 1.4958 - acc: 0.4520 - val_loss: 1.5279 - val_acc: 0.4429\n",
      "Epoch 24/50\n",
      "50000/50000 [==============================] - 55s 1ms/step - loss: 1.5049 - acc: 0.4490 - val_loss: 1.4951 - val_acc: 0.4603\n",
      "Epoch 25/50\n",
      "50000/50000 [==============================] - 55s 1ms/step - loss: 1.4902 - acc: 0.4570 - val_loss: 1.6023 - val_acc: 0.4249\n",
      "Epoch 26/50\n",
      "50000/50000 [==============================] - 55s 1ms/step - loss: 1.4819 - acc: 0.4584 - val_loss: 1.5349 - val_acc: 0.4405\n",
      "Epoch 27/50\n",
      "50000/50000 [==============================] - 55s 1ms/step - loss: 1.4743 - acc: 0.4624 - val_loss: 1.4454 - val_acc: 0.4692\n",
      "Epoch 28/50\n",
      "50000/50000 [==============================] - 55s 1ms/step - loss: 1.4646 - acc: 0.4653 - val_loss: 1.5317 - val_acc: 0.4564\n",
      "Epoch 29/50\n",
      "50000/50000 [==============================] - 55s 1ms/step - loss: 1.4588 - acc: 0.4664 - val_loss: 1.5345 - val_acc: 0.4349\n",
      "Epoch 30/50\n",
      "50000/50000 [==============================] - 55s 1ms/step - loss: 1.4553 - acc: 0.4693 - val_loss: 1.5013 - val_acc: 0.4608\n",
      "Epoch 31/50\n",
      "50000/50000 [==============================] - 55s 1ms/step - loss: 1.4765 - acc: 0.4588 - val_loss: 1.5006 - val_acc: 0.4510\n",
      "Epoch 32/50\n",
      "50000/50000 [==============================] - 55s 1ms/step - loss: 1.4661 - acc: 0.4640 - val_loss: 1.4748 - val_acc: 0.4610\n",
      "Epoch 33/50\n",
      "50000/50000 [==============================] - 55s 1ms/step - loss: 1.4549 - acc: 0.4686 - val_loss: 1.4724 - val_acc: 0.4779\n",
      "Epoch 34/50\n",
      "50000/50000 [==============================] - 55s 1ms/step - loss: 1.4308 - acc: 0.4728 - val_loss: 1.4569 - val_acc: 0.4551\n",
      "Epoch 35/50\n",
      "50000/50000 [==============================] - 55s 1ms/step - loss: 1.4241 - acc: 0.4796 - val_loss: 1.4794 - val_acc: 0.4553\n",
      "Epoch 36/50\n",
      "50000/50000 [==============================] - 55s 1ms/step - loss: 1.4176 - acc: 0.4804 - val_loss: 1.5032 - val_acc: 0.4521\n",
      "Epoch 37/50\n",
      "50000/50000 [==============================] - 55s 1ms/step - loss: 1.4299 - acc: 0.4786 - val_loss: 1.5006 - val_acc: 0.4512\n",
      "Epoch 38/50\n",
      "50000/50000 [==============================] - 55s 1ms/step - loss: 1.4199 - acc: 0.4823 - val_loss: 1.4700 - val_acc: 0.4690\n",
      "Epoch 39/50\n",
      "50000/50000 [==============================] - 55s 1ms/step - loss: 1.4155 - acc: 0.4846 - val_loss: 1.5519 - val_acc: 0.4311\n",
      "Epoch 40/50\n",
      "50000/50000 [==============================] - 55s 1ms/step - loss: 1.4128 - acc: 0.4851 - val_loss: 1.5056 - val_acc: 0.4682\n",
      "Epoch 41/50\n",
      "50000/50000 [==============================] - 55s 1ms/step - loss: 1.4164 - acc: 0.4829 - val_loss: 1.4964 - val_acc: 0.4528\n",
      "Epoch 42/50\n",
      "50000/50000 [==============================] - 55s 1ms/step - loss: 1.3972 - acc: 0.4905 - val_loss: 1.4822 - val_acc: 0.4635\n",
      "Epoch 43/50\n",
      "50000/50000 [==============================] - 55s 1ms/step - loss: 1.4001 - acc: 0.4878 - val_loss: 2.2569 - val_acc: 0.2903\n",
      "Epoch 44/50\n",
      "50000/50000 [==============================] - 55s 1ms/step - loss: 1.4382 - acc: 0.4738 - val_loss: 1.4613 - val_acc: 0.4618\n",
      "Epoch 45/50\n",
      "50000/50000 [==============================] - 55s 1ms/step - loss: 1.4286 - acc: 0.4799 - val_loss: 1.4458 - val_acc: 0.4729\n",
      "Epoch 46/50\n",
      "50000/50000 [==============================] - 55s 1ms/step - loss: 1.3872 - acc: 0.4936 - val_loss: 1.4937 - val_acc: 0.4541\n",
      "Epoch 47/50\n",
      "50000/50000 [==============================] - 55s 1ms/step - loss: 1.3775 - acc: 0.4983 - val_loss: 1.4508 - val_acc: 0.4795\n",
      "Epoch 48/50\n",
      "50000/50000 [==============================] - 55s 1ms/step - loss: 1.3722 - acc: 0.4991 - val_loss: 1.4908 - val_acc: 0.4764\n",
      "Epoch 49/50\n",
      "50000/50000 [==============================] - 55s 1ms/step - loss: 1.3767 - acc: 0.4980 - val_loss: 1.4579 - val_acc: 0.4798\n",
      "Epoch 50/50\n",
      "50000/50000 [==============================] - 55s 1ms/step - loss: 1.3648 - acc: 0.5012 - val_loss: 1.5073 - val_acc: 0.4520\n"
     ]
    }
   ],
   "source": [
    "alexnet_model = create_alexnet_model(need_to_train = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-07T05:51:39.546394Z",
     "start_time": "2019-03-07T05:51:38.191Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 356
    },
    "colab_type": "code",
    "id": "i7nV1CVxG6wo",
    "outputId": "8a5ee238-6ac0-42f6-8ea9-58bc69f81865"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 5s 470us/step\n",
      "Test loss: 1.5073077793121339\n",
      "Test accuracy: 0.452\n",
      "['loss', 'acc']\n",
      "[1.5073077793121339, 0.452]\n"
     ]
    }
   ],
   "source": [
    "print_model_results(alexnet_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 56
    },
    "colab_type": "code",
    "id": "zTwYRNO4aWOF",
    "outputId": "6563161a-d220-4aae-bebe-d17497b9c8ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved trained model at /content/gdrive/My Drive/Colab Notebooks/saved_models/alexnet_model \n"
     ]
    }
   ],
   "source": [
    "save_model('alexnet_model', model = alexnet_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KrVB0TchG6xI"
   },
   "source": [
    "## Improving AlexNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Pv9JDbYHG6xJ"
   },
   "source": [
    "As you can see, AlexNet does not work very well on such a small dataset. Recall what you have learned from this class to improve its performance. You can also take a look at the [Dropout technique](https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf) and its [implementation in Keras](https://keras.io/layers/core/#dropout). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u4D2L7FbG6xO"
   },
   "outputs": [],
   "source": [
    " def lr_schedule(epoch):\n",
    "    lrate = 0.001\n",
    "    if epoch > 75:\n",
    "        lrate = 0.0005\n",
    "    if epoch > 100:\n",
    "        lrate = 0.0003\n",
    "    return lrate\n",
    " \n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "# preprocessing for new configuration\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    " \n",
    "#z-score\n",
    "mean = np.mean(x_train,axis=(0,1,2,3))\n",
    "std = np.std(x_train,axis=(0,1,2,3))\n",
    "x_train = (x_train-mean)/(std+1e-7)\n",
    "x_test = (x_test-mean)/(std+1e-7)\n",
    " \n",
    "num_classes = 10\n",
    "y_train = np_utils.to_categorical(y_train,num_classes)\n",
    "y_test = np_utils.to_categorical(y_test,num_classes)\n",
    " \n",
    "weight_decay = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XtENb_65wAJN"
   },
   "outputs": [],
   "source": [
    "# https://appliedmachinelearning.blog/2018/03/24/achieving-90-accuracy-in-object-recognition-task-on-cifar-10-dataset-with-keras-convolutional-neural-networks/\n",
    "def create_advanced_model():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay), input_shape=x_train.shape[1:]))\n",
    "    model.add(Activation('elu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(32, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "    model.add(Activation('elu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Conv2D(64, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "    model.add(Activation('elu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(64, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "    model.add(Activation('elu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    model.add(Conv2D(128, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "    model.add(Activation('elu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(128, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "    model.add(Activation('elu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Dropout(0.4))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    #data augmentation\n",
    "    datagen = ImageDataGenerator(\n",
    "        rotation_range=15,\n",
    "        width_shift_range=0.1,\n",
    "        height_shift_range=0.1,\n",
    "        horizontal_flip=True,\n",
    "        )\n",
    "    datagen.fit(x_train)\n",
    "\n",
    "    #training\n",
    "    batch_size = 64\n",
    "\n",
    "    opt_rms = keras.optimizers.rmsprop(lr=0.001,decay=1e-6)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=opt_rms, metrics=['accuracy'])\n",
    "    model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),\\\n",
    "                        steps_per_epoch=x_train.shape[0] // batch_size,epochs=125,\\\n",
    "                        verbose=1,validation_data=(x_test,y_test),callbacks=[LearningRateScheduler(lr_schedule)])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 5476
    },
    "colab_type": "code",
    "id": "B5bYgmygjPlj",
    "outputId": "d5f544fc-dfaa-4b65-fc23-3ec88b82be79"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_34 (Conv2D)           (None, 32, 32, 32)        896       \n",
      "_________________________________________________________________\n",
      "activation_38 (Activation)   (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_28 (Batc (None, 32, 32, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_35 (Conv2D)           (None, 32, 32, 32)        9248      \n",
      "_________________________________________________________________\n",
      "activation_39 (Activation)   (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_29 (Batc (None, 32, 32, 32)        128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_18 (MaxPooling (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_36 (Conv2D)           (None, 16, 16, 64)        18496     \n",
      "_________________________________________________________________\n",
      "activation_40 (Activation)   (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_30 (Batc (None, 16, 16, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_37 (Conv2D)           (None, 16, 16, 64)        36928     \n",
      "_________________________________________________________________\n",
      "activation_41 (Activation)   (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_31 (Batc (None, 16, 16, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_19 (MaxPooling (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_38 (Conv2D)           (None, 8, 8, 128)         73856     \n",
      "_________________________________________________________________\n",
      "activation_42 (Activation)   (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_32 (Batc (None, 8, 8, 128)         512       \n",
      "_________________________________________________________________\n",
      "conv2d_39 (Conv2D)           (None, 8, 8, 128)         147584    \n",
      "_________________________________________________________________\n",
      "activation_43 (Activation)   (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_33 (Batc (None, 8, 8, 128)         512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_20 (MaxPooling (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 10)                20490     \n",
      "=================================================================\n",
      "Total params: 309,290\n",
      "Trainable params: 308,394\n",
      "Non-trainable params: 896\n",
      "_________________________________________________________________\n",
      "Epoch 1/125\n",
      "781/781 [==============================] - 51s 65ms/step - loss: 1.8901 - acc: 0.4267 - val_loss: 1.2803 - val_acc: 0.5846\n",
      "Epoch 2/125\n",
      "781/781 [==============================] - 46s 59ms/step - loss: 1.2581 - acc: 0.5953 - val_loss: 0.9548 - val_acc: 0.6970\n",
      "Epoch 3/125\n",
      "781/781 [==============================] - 45s 58ms/step - loss: 1.0738 - acc: 0.6566 - val_loss: 1.2223 - val_acc: 0.6448\n",
      "Epoch 4/125\n",
      "781/781 [==============================] - 44s 56ms/step - loss: 0.9721 - acc: 0.6936 - val_loss: 0.8353 - val_acc: 0.7448\n",
      "Epoch 5/125\n",
      "781/781 [==============================] - 45s 58ms/step - loss: 0.9122 - acc: 0.7166 - val_loss: 0.8307 - val_acc: 0.7483\n",
      "Epoch 6/125\n",
      "781/781 [==============================] - 45s 58ms/step - loss: 0.8630 - acc: 0.7359 - val_loss: 0.7955 - val_acc: 0.7657\n",
      "Epoch 7/125\n",
      "781/781 [==============================] - 45s 58ms/step - loss: 0.8293 - acc: 0.7499 - val_loss: 0.7339 - val_acc: 0.7902\n",
      "Epoch 8/125\n",
      "781/781 [==============================] - 45s 58ms/step - loss: 0.7960 - acc: 0.7629 - val_loss: 0.7600 - val_acc: 0.7825\n",
      "Epoch 9/125\n",
      "781/781 [==============================] - 46s 59ms/step - loss: 0.7810 - acc: 0.7691 - val_loss: 0.7162 - val_acc: 0.7988\n",
      "Epoch 10/125\n",
      "781/781 [==============================] - 46s 59ms/step - loss: 0.7550 - acc: 0.7803 - val_loss: 0.9161 - val_acc: 0.7512\n",
      "Epoch 11/125\n",
      "781/781 [==============================] - 46s 59ms/step - loss: 0.7433 - acc: 0.7843 - val_loss: 0.6639 - val_acc: 0.8180\n",
      "Epoch 12/125\n",
      "781/781 [==============================] - 46s 59ms/step - loss: 0.7304 - acc: 0.7908 - val_loss: 0.7128 - val_acc: 0.8026\n",
      "Epoch 13/125\n",
      "781/781 [==============================] - 45s 57ms/step - loss: 0.7191 - acc: 0.7949 - val_loss: 0.6786 - val_acc: 0.8096\n",
      "Epoch 14/125\n",
      "781/781 [==============================] - 45s 57ms/step - loss: 0.7077 - acc: 0.7994 - val_loss: 0.6747 - val_acc: 0.8177\n",
      "Epoch 15/125\n",
      "781/781 [==============================] - 41s 53ms/step - loss: 0.7003 - acc: 0.8043 - val_loss: 0.7681 - val_acc: 0.7954\n",
      "Epoch 16/125\n",
      "781/781 [==============================] - 42s 53ms/step - loss: 0.6896 - acc: 0.8077 - val_loss: 0.6894 - val_acc: 0.8184\n",
      "Epoch 17/125\n",
      "781/781 [==============================] - 42s 54ms/step - loss: 0.6845 - acc: 0.8096 - val_loss: 0.6170 - val_acc: 0.8406\n",
      "Epoch 18/125\n",
      "781/781 [==============================] - 43s 54ms/step - loss: 0.6767 - acc: 0.8129 - val_loss: 0.6976 - val_acc: 0.8165\n",
      "Epoch 19/125\n",
      "781/781 [==============================] - 42s 54ms/step - loss: 0.6703 - acc: 0.8161 - val_loss: 0.7132 - val_acc: 0.8160\n",
      "Epoch 20/125\n",
      "781/781 [==============================] - 43s 55ms/step - loss: 0.6618 - acc: 0.8181 - val_loss: 0.6901 - val_acc: 0.8183\n",
      "Epoch 21/125\n",
      "781/781 [==============================] - 42s 54ms/step - loss: 0.6564 - acc: 0.8223 - val_loss: 0.6469 - val_acc: 0.8365\n",
      "Epoch 22/125\n",
      "781/781 [==============================] - 45s 58ms/step - loss: 0.6521 - acc: 0.8218 - val_loss: 0.6412 - val_acc: 0.8356\n",
      "Epoch 23/125\n",
      "781/781 [==============================] - 45s 58ms/step - loss: 0.6503 - acc: 0.8236 - val_loss: 0.6663 - val_acc: 0.8296\n",
      "Epoch 24/125\n",
      "781/781 [==============================] - 45s 57ms/step - loss: 0.6457 - acc: 0.8256 - val_loss: 0.7534 - val_acc: 0.8067\n",
      "Epoch 25/125\n",
      "781/781 [==============================] - 45s 58ms/step - loss: 0.6440 - acc: 0.8302 - val_loss: 0.6521 - val_acc: 0.8341\n",
      "Epoch 26/125\n",
      "781/781 [==============================] - 46s 59ms/step - loss: 0.6408 - acc: 0.8282 - val_loss: 0.5746 - val_acc: 0.8555\n",
      "Epoch 27/125\n",
      "781/781 [==============================] - 47s 60ms/step - loss: 0.6350 - acc: 0.8303 - val_loss: 0.6355 - val_acc: 0.8406\n",
      "Epoch 28/125\n",
      "781/781 [==============================] - 46s 59ms/step - loss: 0.6315 - acc: 0.8318 - val_loss: 0.5793 - val_acc: 0.8559\n",
      "Epoch 29/125\n",
      "781/781 [==============================] - 46s 59ms/step - loss: 0.6281 - acc: 0.8333 - val_loss: 0.6586 - val_acc: 0.8325\n",
      "Epoch 30/125\n",
      "781/781 [==============================] - 46s 59ms/step - loss: 0.6272 - acc: 0.8323 - val_loss: 0.5959 - val_acc: 0.8532\n",
      "Epoch 31/125\n",
      "781/781 [==============================] - 46s 59ms/step - loss: 0.6248 - acc: 0.8363 - val_loss: 0.6128 - val_acc: 0.8480\n",
      "Epoch 32/125\n",
      "781/781 [==============================] - 46s 59ms/step - loss: 0.6208 - acc: 0.8359 - val_loss: 0.6159 - val_acc: 0.8432\n",
      "Epoch 33/125\n",
      "781/781 [==============================] - 46s 59ms/step - loss: 0.6212 - acc: 0.8353 - val_loss: 0.6178 - val_acc: 0.8480\n",
      "Epoch 34/125\n",
      "781/781 [==============================] - 46s 58ms/step - loss: 0.6140 - acc: 0.8377 - val_loss: 0.6528 - val_acc: 0.8339\n",
      "Epoch 35/125\n",
      "781/781 [==============================] - 45s 58ms/step - loss: 0.6162 - acc: 0.8395 - val_loss: 0.5662 - val_acc: 0.8633\n",
      "Epoch 36/125\n",
      "781/781 [==============================] - 45s 58ms/step - loss: 0.6148 - acc: 0.8374 - val_loss: 0.6198 - val_acc: 0.8462\n",
      "Epoch 37/125\n",
      "781/781 [==============================] - 46s 59ms/step - loss: 0.6202 - acc: 0.8380 - val_loss: 0.5930 - val_acc: 0.8534\n",
      "Epoch 38/125\n",
      "781/781 [==============================] - 45s 58ms/step - loss: 0.6040 - acc: 0.8421 - val_loss: 0.5840 - val_acc: 0.8568\n",
      "Epoch 39/125\n",
      "781/781 [==============================] - 45s 58ms/step - loss: 0.6094 - acc: 0.8417 - val_loss: 0.6894 - val_acc: 0.8245\n",
      "Epoch 40/125\n",
      "781/781 [==============================] - 43s 56ms/step - loss: 0.6088 - acc: 0.8417 - val_loss: 0.6491 - val_acc: 0.8347\n",
      "Epoch 41/125\n",
      "781/781 [==============================] - 44s 56ms/step - loss: 0.6097 - acc: 0.8417 - val_loss: 0.6117 - val_acc: 0.8519\n",
      "Epoch 42/125\n",
      "781/781 [==============================] - 44s 56ms/step - loss: 0.6016 - acc: 0.8426 - val_loss: 0.6472 - val_acc: 0.8382\n",
      "Epoch 43/125\n",
      "781/781 [==============================] - 44s 56ms/step - loss: 0.6055 - acc: 0.8417 - val_loss: 0.6145 - val_acc: 0.8536\n",
      "Epoch 44/125\n",
      "781/781 [==============================] - 41s 52ms/step - loss: 0.5979 - acc: 0.8449 - val_loss: 0.7287 - val_acc: 0.8166\n",
      "Epoch 45/125\n",
      "781/781 [==============================] - 41s 53ms/step - loss: 0.5989 - acc: 0.8466 - val_loss: 0.5706 - val_acc: 0.8616\n",
      "Epoch 46/125\n",
      "781/781 [==============================] - 42s 54ms/step - loss: 0.5963 - acc: 0.8470 - val_loss: 0.5948 - val_acc: 0.8522\n",
      "Epoch 47/125\n",
      "781/781 [==============================] - 46s 59ms/step - loss: 0.5981 - acc: 0.8455 - val_loss: 0.6005 - val_acc: 0.8504\n",
      "Epoch 48/125\n",
      "781/781 [==============================] - 46s 59ms/step - loss: 0.5942 - acc: 0.8479 - val_loss: 0.6619 - val_acc: 0.8347\n",
      "Epoch 49/125\n",
      "781/781 [==============================] - 46s 59ms/step - loss: 0.5936 - acc: 0.8490 - val_loss: 0.5962 - val_acc: 0.8551\n",
      "Epoch 50/125\n",
      "781/781 [==============================] - 46s 59ms/step - loss: 0.5926 - acc: 0.8493 - val_loss: 0.6062 - val_acc: 0.8574\n",
      "Epoch 51/125\n",
      "781/781 [==============================] - 46s 59ms/step - loss: 0.5940 - acc: 0.8474 - val_loss: 0.5477 - val_acc: 0.8675\n",
      "Epoch 52/125\n",
      "781/781 [==============================] - 46s 59ms/step - loss: 0.5911 - acc: 0.8481 - val_loss: 0.5823 - val_acc: 0.8586\n",
      "Epoch 53/125\n",
      "781/781 [==============================] - 46s 58ms/step - loss: 0.5866 - acc: 0.8491 - val_loss: 0.6929 - val_acc: 0.8327\n",
      "Epoch 54/125\n",
      "781/781 [==============================] - 45s 58ms/step - loss: 0.5964 - acc: 0.8475 - val_loss: 0.6243 - val_acc: 0.8447\n",
      "Epoch 55/125\n",
      "781/781 [==============================] - 45s 58ms/step - loss: 0.5855 - acc: 0.8519 - val_loss: 0.6318 - val_acc: 0.8437\n",
      "Epoch 56/125\n",
      "781/781 [==============================] - 45s 58ms/step - loss: 0.5877 - acc: 0.8483 - val_loss: 0.6830 - val_acc: 0.8337\n",
      "Epoch 57/125\n",
      "781/781 [==============================] - 45s 57ms/step - loss: 0.5886 - acc: 0.8480 - val_loss: 0.6623 - val_acc: 0.8363\n",
      "Epoch 58/125\n",
      "781/781 [==============================] - 45s 57ms/step - loss: 0.5849 - acc: 0.8510 - val_loss: 0.5587 - val_acc: 0.8627\n",
      "Epoch 59/125\n",
      "781/781 [==============================] - 45s 58ms/step - loss: 0.5848 - acc: 0.8515 - val_loss: 0.5556 - val_acc: 0.8659\n",
      "Epoch 60/125\n",
      "781/781 [==============================] - 46s 59ms/step - loss: 0.5856 - acc: 0.8503 - val_loss: 0.5955 - val_acc: 0.8570\n",
      "Epoch 61/125\n",
      "781/781 [==============================] - 46s 59ms/step - loss: 0.5814 - acc: 0.8514 - val_loss: 0.5832 - val_acc: 0.8590\n",
      "Epoch 62/125\n",
      "781/781 [==============================] - 46s 59ms/step - loss: 0.5841 - acc: 0.8518 - val_loss: 0.6062 - val_acc: 0.8507\n",
      "Epoch 63/125\n",
      "781/781 [==============================] - 46s 59ms/step - loss: 0.5768 - acc: 0.8535 - val_loss: 0.5891 - val_acc: 0.8569\n",
      "Epoch 64/125\n",
      "781/781 [==============================] - 46s 59ms/step - loss: 0.5808 - acc: 0.8528 - val_loss: 0.5578 - val_acc: 0.8687\n",
      "Epoch 65/125\n",
      "781/781 [==============================] - 46s 58ms/step - loss: 0.5815 - acc: 0.8528 - val_loss: 0.6389 - val_acc: 0.8458\n",
      "Epoch 66/125\n",
      "781/781 [==============================] - 46s 59ms/step - loss: 0.5751 - acc: 0.8547 - val_loss: 0.6679 - val_acc: 0.8408\n",
      "Epoch 67/125\n",
      "781/781 [==============================] - 46s 58ms/step - loss: 0.5796 - acc: 0.8527 - val_loss: 0.6078 - val_acc: 0.8524\n",
      "Epoch 68/125\n",
      "781/781 [==============================] - 46s 59ms/step - loss: 0.5800 - acc: 0.8544 - val_loss: 0.6283 - val_acc: 0.8452\n",
      "Epoch 69/125\n",
      "781/781 [==============================] - 46s 59ms/step - loss: 0.5790 - acc: 0.8532 - val_loss: 0.5298 - val_acc: 0.8774\n",
      "Epoch 70/125\n",
      "781/781 [==============================] - 46s 59ms/step - loss: 0.5808 - acc: 0.8531 - val_loss: 0.6271 - val_acc: 0.8422\n",
      "Epoch 71/125\n",
      "781/781 [==============================] - 46s 59ms/step - loss: 0.5715 - acc: 0.8572 - val_loss: 0.5412 - val_acc: 0.8741\n",
      "Epoch 72/125\n",
      "781/781 [==============================] - 46s 59ms/step - loss: 0.5766 - acc: 0.8543 - val_loss: 0.5867 - val_acc: 0.8645\n",
      "Epoch 73/125\n",
      "781/781 [==============================] - 46s 59ms/step - loss: 0.5751 - acc: 0.8528 - val_loss: 0.5482 - val_acc: 0.8697\n",
      "Epoch 74/125\n",
      "781/781 [==============================] - 45s 58ms/step - loss: 0.5716 - acc: 0.8568 - val_loss: 0.5894 - val_acc: 0.8597\n",
      "Epoch 75/125\n",
      "781/781 [==============================] - 45s 57ms/step - loss: 0.5712 - acc: 0.8561 - val_loss: 0.6132 - val_acc: 0.8508\n",
      "Epoch 76/125\n",
      "781/781 [==============================] - 45s 57ms/step - loss: 0.5722 - acc: 0.8546 - val_loss: 0.5801 - val_acc: 0.8623\n",
      "Epoch 77/125\n",
      "781/781 [==============================] - 45s 57ms/step - loss: 0.5260 - acc: 0.8711 - val_loss: 0.5445 - val_acc: 0.8710\n",
      "Epoch 78/125\n",
      "781/781 [==============================] - 43s 56ms/step - loss: 0.5102 - acc: 0.8752 - val_loss: 0.5457 - val_acc: 0.8681\n",
      "Epoch 79/125\n",
      "781/781 [==============================] - 41s 53ms/step - loss: 0.5026 - acc: 0.8767 - val_loss: 0.5150 - val_acc: 0.8770\n",
      "Epoch 80/125\n",
      "781/781 [==============================] - 39s 50ms/step - loss: 0.4976 - acc: 0.8770 - val_loss: 0.5078 - val_acc: 0.8797\n",
      "Epoch 81/125\n",
      "781/781 [==============================] - 37s 47ms/step - loss: 0.4939 - acc: 0.8769 - val_loss: 0.5330 - val_acc: 0.8736\n",
      "Epoch 82/125\n",
      "781/781 [==============================] - 37s 48ms/step - loss: 0.4921 - acc: 0.8770 - val_loss: 0.5559 - val_acc: 0.8668\n",
      "Epoch 83/125\n",
      "781/781 [==============================] - 40s 51ms/step - loss: 0.4835 - acc: 0.8797 - val_loss: 0.5080 - val_acc: 0.8796\n",
      "Epoch 84/125\n",
      "781/781 [==============================] - 44s 56ms/step - loss: 0.4832 - acc: 0.8793 - val_loss: 0.4869 - val_acc: 0.8829\n",
      "Epoch 85/125\n",
      "781/781 [==============================] - 44s 56ms/step - loss: 0.4791 - acc: 0.8789 - val_loss: 0.5169 - val_acc: 0.8760\n",
      "Epoch 86/125\n",
      "781/781 [==============================] - 43s 55ms/step - loss: 0.4781 - acc: 0.8795 - val_loss: 0.5263 - val_acc: 0.8763\n",
      "Epoch 87/125\n",
      "781/781 [==============================] - 40s 51ms/step - loss: 0.4712 - acc: 0.8804 - val_loss: 0.5343 - val_acc: 0.8645\n",
      "Epoch 88/125\n",
      "781/781 [==============================] - 40s 52ms/step - loss: 0.4721 - acc: 0.8793 - val_loss: 0.5203 - val_acc: 0.8727\n",
      "Epoch 89/125\n",
      "781/781 [==============================] - 45s 57ms/step - loss: 0.4678 - acc: 0.8820 - val_loss: 0.4982 - val_acc: 0.8764\n",
      "Epoch 90/125\n",
      "781/781 [==============================] - 45s 57ms/step - loss: 0.4618 - acc: 0.8820 - val_loss: 0.5659 - val_acc: 0.8636\n",
      "Epoch 91/125\n",
      "781/781 [==============================] - 44s 56ms/step - loss: 0.4643 - acc: 0.8822 - val_loss: 0.4846 - val_acc: 0.8862\n",
      "Epoch 92/125\n",
      "781/781 [==============================] - 43s 56ms/step - loss: 0.4621 - acc: 0.8814 - val_loss: 0.4688 - val_acc: 0.8896\n",
      "Epoch 93/125\n",
      "781/781 [==============================] - 43s 55ms/step - loss: 0.4688 - acc: 0.8795 - val_loss: 0.5712 - val_acc: 0.8592\n",
      "Epoch 94/125\n",
      "781/781 [==============================] - 43s 56ms/step - loss: 0.4602 - acc: 0.8827 - val_loss: 0.5031 - val_acc: 0.8781\n",
      "Epoch 95/125\n",
      "781/781 [==============================] - 42s 54ms/step - loss: 0.4660 - acc: 0.8802 - val_loss: 0.5480 - val_acc: 0.8621\n",
      "Epoch 96/125\n",
      "781/781 [==============================] - 43s 55ms/step - loss: 0.4552 - acc: 0.8838 - val_loss: 0.4992 - val_acc: 0.8764\n",
      "Epoch 97/125\n",
      "781/781 [==============================] - 42s 53ms/step - loss: 0.4553 - acc: 0.8836 - val_loss: 0.4737 - val_acc: 0.8848\n",
      "Epoch 98/125\n",
      "781/781 [==============================] - 38s 49ms/step - loss: 0.4583 - acc: 0.8819 - val_loss: 0.5194 - val_acc: 0.8737\n",
      "Epoch 99/125\n",
      "781/781 [==============================] - 40s 52ms/step - loss: 0.4547 - acc: 0.8827 - val_loss: 0.4877 - val_acc: 0.8813\n",
      "Epoch 100/125\n",
      "781/781 [==============================] - 39s 50ms/step - loss: 0.4507 - acc: 0.8833 - val_loss: 0.5018 - val_acc: 0.8783\n",
      "Epoch 101/125\n",
      "781/781 [==============================] - 39s 50ms/step - loss: 0.4512 - acc: 0.8845 - val_loss: 0.4879 - val_acc: 0.8814\n",
      "Epoch 102/125\n",
      "781/781 [==============================] - 38s 48ms/step - loss: 0.4298 - acc: 0.8914 - val_loss: 0.4773 - val_acc: 0.8830\n",
      "Epoch 103/125\n",
      "781/781 [==============================] - 43s 55ms/step - loss: 0.4226 - acc: 0.8933 - val_loss: 0.4680 - val_acc: 0.8879\n",
      "Epoch 104/125\n",
      "781/781 [==============================] - 44s 57ms/step - loss: 0.4230 - acc: 0.8921 - val_loss: 0.4710 - val_acc: 0.8837\n",
      "Epoch 105/125\n",
      "781/781 [==============================] - 45s 58ms/step - loss: 0.4207 - acc: 0.8937 - val_loss: 0.4594 - val_acc: 0.8887\n",
      "Epoch 106/125\n",
      "781/781 [==============================] - 46s 60ms/step - loss: 0.4213 - acc: 0.8925 - val_loss: 0.4559 - val_acc: 0.8880\n",
      "Epoch 107/125\n",
      "781/781 [==============================] - 46s 59ms/step - loss: 0.4134 - acc: 0.8942 - val_loss: 0.4538 - val_acc: 0.8866\n",
      "Epoch 108/125\n",
      "781/781 [==============================] - 46s 59ms/step - loss: 0.4077 - acc: 0.8968 - val_loss: 0.4986 - val_acc: 0.8753\n",
      "Epoch 109/125\n",
      "781/781 [==============================] - 46s 59ms/step - loss: 0.4087 - acc: 0.8947 - val_loss: 0.4653 - val_acc: 0.8851\n",
      "Epoch 110/125\n",
      "781/781 [==============================] - 46s 59ms/step - loss: 0.4068 - acc: 0.8962 - val_loss: 0.4406 - val_acc: 0.8949\n",
      "Epoch 111/125\n",
      "781/781 [==============================] - 47s 60ms/step - loss: 0.4127 - acc: 0.8943 - val_loss: 0.4256 - val_acc: 0.8966\n",
      "Epoch 112/125\n",
      "781/781 [==============================] - 46s 59ms/step - loss: 0.4056 - acc: 0.8955 - val_loss: 0.4640 - val_acc: 0.8859\n",
      "Epoch 113/125\n",
      "781/781 [==============================] - 46s 59ms/step - loss: 0.4040 - acc: 0.8965 - val_loss: 0.4549 - val_acc: 0.8908\n",
      "Epoch 114/125\n",
      "781/781 [==============================] - 46s 59ms/step - loss: 0.4035 - acc: 0.8966 - val_loss: 0.4557 - val_acc: 0.8873\n",
      "Epoch 115/125\n",
      "781/781 [==============================] - 46s 59ms/step - loss: 0.4039 - acc: 0.8955 - val_loss: 0.4503 - val_acc: 0.8905\n",
      "Epoch 116/125\n",
      "781/781 [==============================] - 46s 59ms/step - loss: 0.4054 - acc: 0.8951 - val_loss: 0.4566 - val_acc: 0.8881\n",
      "Epoch 117/125\n",
      "781/781 [==============================] - 46s 59ms/step - loss: 0.4003 - acc: 0.8958 - val_loss: 0.4451 - val_acc: 0.8880\n",
      "Epoch 118/125\n",
      "781/781 [==============================] - 46s 59ms/step - loss: 0.3974 - acc: 0.8976 - val_loss: 0.4545 - val_acc: 0.8885\n",
      "Epoch 119/125\n",
      "781/781 [==============================] - 46s 59ms/step - loss: 0.3991 - acc: 0.8969 - val_loss: 0.4273 - val_acc: 0.8954\n",
      "Epoch 120/125\n",
      "781/781 [==============================] - 46s 59ms/step - loss: 0.3943 - acc: 0.8968 - val_loss: 0.4525 - val_acc: 0.8876\n",
      "Epoch 121/125\n",
      "781/781 [==============================] - 46s 59ms/step - loss: 0.3974 - acc: 0.8959 - val_loss: 0.4502 - val_acc: 0.8894\n",
      "Epoch 122/125\n",
      "781/781 [==============================] - 46s 59ms/step - loss: 0.3930 - acc: 0.8983 - val_loss: 0.4279 - val_acc: 0.8924\n",
      "Epoch 123/125\n",
      "781/781 [==============================] - 46s 59ms/step - loss: 0.3932 - acc: 0.8980 - val_loss: 0.4739 - val_acc: 0.8819\n",
      "Epoch 124/125\n",
      "781/781 [==============================] - 46s 58ms/step - loss: 0.3914 - acc: 0.8969 - val_loss: 0.4370 - val_acc: 0.8901\n",
      "Epoch 125/125\n",
      "781/781 [==============================] - 46s 58ms/step - loss: 0.3912 - acc: 0.8993 - val_loss: 0.4611 - val_acc: 0.8850\n"
     ]
    }
   ],
   "source": [
    "advanced_model = create_advanced_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 116
    },
    "colab_type": "code",
    "id": "p02jEjqJjSpA",
    "outputId": "cf54f693-f9b8-4c24-cef5-1b7cbf916edd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 141us/step\n",
      "Test loss: 0.46111083664894104\n",
      "Test accuracy: 0.885\n",
      "[0.46111083664894104, 0.885]\n"
     ]
    }
   ],
   "source": [
    "score = advanced_model.evaluate(x_test, y_test, batch_size=128, verbose=1)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "print(score)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Problem Set 2 – Image recognition.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
