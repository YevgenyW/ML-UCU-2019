{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In class, we have briefly reviewed the idea of learning good features directly from data and went through the concept of convolutional neural networks along with few architectures.\n",
    "\n",
    "Until recently, building convolutional neural networks was tough. There was no high-level tools for that, you would be required to understand all the internal mechanics of the model and its operations.\n",
    "\n",
    "Today, due to the high-level tools such as Keras and TensorFlow, everybody can build a convolutional neural network and put it to work without diving deep into them. What used to be a one-month project became a few hours exercise.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-06T16:53:21.353576Z",
     "start_time": "2019-03-06T16:53:21.349395Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-06T16:53:21.760452Z",
     "start_time": "2019-03-06T16:53:21.356396Z"
    }
   },
   "outputs": [],
   "source": [
    "train_images, train_labels = pickle.load(open('data/train_set_all.pkl', 'rb'))\n",
    "cv_images, cv_labels = pickle.load(open('data/test_set_all.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-06T16:53:21.766228Z",
     "start_time": "2019-03-06T16:53:21.762448Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 32, 32, 3)\n",
      "50000\n"
     ]
    }
   ],
   "source": [
    "print(train_images.shape)\n",
    "print(len(train_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-06T16:53:21.772667Z",
     "start_time": "2019-03-06T16:53:21.769159Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 32, 32, 3)\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "print(cv_images.shape)\n",
    "print(len(cv_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-06T16:53:21.780026Z",
     "start_time": "2019-03-06T16:53:21.776724Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6, 9, 9, 4, 1, 1, 2, 7, 8, 3]\n"
     ]
    }
   ],
   "source": [
    "print(train_labels[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-06T16:53:21.801456Z",
     "start_time": "2019-03-06T16:53:21.791613Z"
    }
   },
   "outputs": [],
   "source": [
    "# functions to save and load model weights\n",
    "def load_model(model_name, model):\n",
    "    dir = os.path.join(os.getcwd(), 'saved_models')\n",
    "    if not os.path.isdir(save_dir):\n",
    "        return None\n",
    "    model_path = os.path.join(save_dir, model_name)\n",
    "    model.load_weights(model_path)\n",
    "    return model\n",
    "    \n",
    "def save_model(model_name, model):\n",
    "    save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
    "    # Save model and weights\n",
    "    if not os.path.isdir(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    model_path = os.path.join(save_dir, model_name)\n",
    "    model.save(model_path)\n",
    "    print('Saved trained model at %s ' % model_path)\n",
    "    \n",
    "# def get_model_with_weights(model_func, need_to_train = False):\n",
    "#     model = model_func()\n",
    "#     if need_to_train == True:\n",
    "#         model.fit(train_images, Y_train, \n",
    "#               batch_size=32, nb_epoch=10, verbose=1)\n",
    "#         save_model(model_name = 'model_for_mnist')\n",
    "#     else:\n",
    "#         model = load_model('model_for_mnist', model)\n",
    "#     return model\n",
    "\n",
    "def print_model_results(model):\n",
    "    score = model.evaluate(cv_images, Y_cv, batch_size=32, verbose=1)\n",
    "    print('Test loss:', score[0])\n",
    "    print('Test accuracy:', score[1])\n",
    "    print(model.metrics_names)\n",
    "    print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-06T16:53:21.812981Z",
     "start_time": "2019-03-06T16:53:21.808133Z"
    }
   },
   "outputs": [],
   "source": [
    "def show_image(np_array):\n",
    "    %matplotlib inline\n",
    "    plt.figure()\n",
    "    plt.imshow(np_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-06T16:53:21.817655Z",
     "start_time": "2019-03-06T16:53:21.814781Z"
    }
   },
   "outputs": [],
   "source": [
    "def show_example(data_set, labels, example_index):\n",
    "    show_image(data_set[example_index])\n",
    "    print('Label: ', labels[example_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-06T16:53:21.995910Z",
     "start_time": "2019-03-06T16:53:21.820841Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label:  6\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_example(train_images, train_labels, example_index = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-06T16:53:22.161598Z",
     "start_time": "2019-03-06T16:53:21.998077Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label:  3\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAHzFJREFUeJztnWuMXdd13//rnvuYufPkcPgWxZcetqzIkkyzjpW6juOqihNAVpEY9gdBH9wwKGKgBtICggvUDtAPTlHb8IfCBV0JUQq/FD9qtXWcGIodxbUhiVIk6kFJpGi+h5wZct6P+1z9MFcANd7/PZcc8g6V/f8BBO/sdfY5++x71j337v9Za5m7QwiRHrm1HoAQYm2Q8wuRKHJ+IRJFzi9Eosj5hUgUOb8QiSLnFyJR5PxCJIqcX4hEya+ms5ndB+CrADIA/8PdvxjbvruU+UBP+JA5M9ovR0y8x8pW3o0/8cjGGHtG0q5wHBYZR3yPl//EpkXmPmICnBudjMMjfaJnZs3IOC7fFH1fIqbYE7Gxfcbm+EqOxSwXpquYWai3dbArdn4zywD8NwD/EsBpAM+a2RPu/irrM9CTx4Mf3Ra0lYsZPRb5vEAxx7+4NMH358b7ZVmd2orFQnh/kTcpFzlWzE1LOW7NZ7GrPTz+XGR/hXxkPnL8EvEmv8YaCI+jVu3i+4u8Z1aYo7Zmg5rgzfA+jd1RAGSR66parVJbPgtfHwBQLJaozZvhD7ZqjV+L7Br+s2++RvssZzVf+/cBOOrux9y9CuDbAO5fxf6EEB1kNc6/DcCpS/4+3WoTQrwDWM1v/tD3pl/7bmlm+wHsB4C+Mv9aJ4ToLKu5858GsP2Sv28AcHb5Ru5+wN33uvvecknOL8T1wmqc/1kAN5vZLjMrAvgkgCeuzrCEENeaK/7a7+51M/sMgL/BktT3qLu/EutjALKMyWWRlU22Lm6R1eHIgng+z087B74qi2a4X6XGZSiPrZbn+PjrGd9nMePL21kj3K/YXKR9cpHlciOr5QBgOT5XbDW9ETvnBr8X1WrUBHM+/hJTaJoRhSZyS8xHlJGcR77ZRuaxSt6bRecnbdYTbPfLkJZXpfO7+48A/Gg1+xBCrA16wk+IRJHzC5Eocn4hEkXOL0SiyPmFSJRVrfZfNsaj1aLBJUQl8UikVDMaqVaktrkKn5Lp+Uqwvep87DOLXK5pRCLVenv5PnuKXKbaPNAXbC9k/JzNuAzYbHAJthmR2FjQTzMisTUjUlmW57Ji3iI6IBljlsXue/x9ySIBQY2F8PUBABaN0Fsghtj8svloP6pTd34hEkXOL0SiyPmFSBQ5vxCJIucXIlE6utpvZigVwiu6xXwsgCTc3jQ+/MU639+FizwV08nzs9SWdYVX0mcrkSCc3vXU1jswRG2z4CvwlQWe0mpi8mKw/Td2DNM+3iSrzQCKuchqv/HV7Vo9PMfD6zfTPv2DfIznzr/Bx1GN5PEi6kI+EmAUi41pRpSAfJ6vtJeMz2OlEVYr8rG8f43w3MdUheXozi9Eosj5hUgUOb8QiSLnFyJR5PxCJIqcX4hE6azU5wArHNOocbkmVyoH2+er/LPr/Azf3/g872eDO6htlkhK63dspX3yXb3U1t0blg4BoFiPBIlUZqjt9LGwbPc3vzhK++zaxuXIHTdsoLZ8xiXHAql8NLR1J+3zvg/+JrX99eN8/AuVy5f6GpEuWUR2rhMJE4jL1fVYNaIsLPlGg4GI1Hc56M4vRKLI+YVIFDm/EIki5xciUeT8QiSKnF+IRFmV1GdmxwHMAGgAqLv73hW2R4GUr8pnkRJa+bBcNj7Bo9HeGOFRcQsIS4cAsGvXILUNdIdLJNXqXJJZ1x/uAwCFIj/nLJIXsJ7x4+X7w5FxF2a4LPfqKJeNXjr5GrXt2MJlzB3bBoLtxWOngu0AMHVxlNrm5yeoLQeu2zWb4Sg8FikKAM16RHaO5MhbjEThTTd4DsWJ2fB1EMtbOFCYD7Y3Yye2jKuh8/+2u49fhf0IITqIvvYLkSirdX4H8Ldm9pyZ7b8aAxJCdIbVfu2/x93PmtlGAD8xs9fc/alLN2h9KOwHgMGezpYJEEJwVnXnd/ezrf9HAfwAwL7ANgfcfa+77+3pkvMLcb1wxc5vZj1m1vfWawD3Anj5ag1MCHFtWc2teBOAH9iSvJEH8E13/3G0hzuNssryfCisCtKp8zy6bWyOR1Gt27yR2qqLXD7cPLwu2L64wPssTJyltvJ6nsBz42ae6HJ6lktK1VNhaau4nkfuFbu6qG1+coraskhUohcLwfbT5y7wY01xyXFdH5ffjMh5AFAk47BYqbdIyF+hwK/TWee2/3d4hNoOHwnLsB/64L+gfaok6rNOyqSFuGLnd/djAN57pf2FEGuLpD4hEkXOL0SiyPmFSBQ5vxCJIucXIlE6+9SNASSoD41I8sOR8XCE3ug0r3+WFbkMtTDHI9xQDktDADB1MSxTFSPyT6PG5aveSFTfuh4uv42e47JRtRKu+9bXx2XFYolHnJUjkWWD6/upbd2msG167CTtc2byNLVVcjyCcEPkKi4UwzJgoxGOigOAhnOpzyPRlvNV/p5dnOfXY61IEtQaP+eNA+E+lr1O+yxHd34hEkXOL0SiyPmFSBQ5vxCJIucXIlE6vNqfgxXDK8u1jK+U/urcuWD7xGx4ZRsABjfyUxvo66Y2i+RAmyBBLvkcD6bYunkTtZV7eH6/kdN85fuVFw9RW1YKrxBHFrCxMM3Vj3ykY6HA37Pe3nCZr74yVwiO1/g8nhnnAUG777iJ2hamwzkDLYsE9tR4oFa9zvuV+3jZtn2/yUuRTf79M8H2fDdXCG6949Zge1f3P9A+y9GdX4hEkfMLkShyfiESRc4vRKLI+YVIFDm/EInSUanPkUMNYSnqyAmeK+7YqXBBoPL6G2if9ZH8eN3dPFgllgEt83DQz/p14dJUADC8ISx5AUCD5DMEECkKBdQiOeYmxsJzNbCOz0dvmUt2pYj0mXceELQ4Gx7j1o08N6Fv5bn43vd7PGPcjbt3UtvJYweD7c889Tjtk0XkzVykrNyGLTdSmxW4HPmusXDg2o7dt9A+5YFwTsbY+H5t27a3FEL8k0LOL0SiyPmFSBQ5vxCJIucXIlHk/EIkyoq6gJk9CuD3AYy6++2ttiEA3wGwE8BxAJ9w94mV9lWrN3F6NBxBdvQUL71lxbBM1V0epH2yLCJRRSK6BvrDJbkAYPJCOLLMcjz/4MZNXNrKRQS9oZ07qe3Qa29S24lzR4LtGyLjWFzk+ex279lDbYhIjl3FsJzaTfLVAUBXnues6+vhJdZQ5pJvcV34PTsRud5u3s7dol7l71lW4ufW08fLpd35vvcH24+9eZT2uev2ncF2y7V/P29ny78AcN+ytocBPOnuNwN4svW3EOIdxIrO7+5PAbi4rPl+AI+1Xj8G4ONXeVxCiGvMlf7m3+TuIwDQ+j/ynUwIcT1yzR/vNbP9APYDQG83z4kvhOgsV3rnP29mWwCg9f8o29DdD7j7Xnff213qbNYwIQTnSp3/CQAPtV4/BOCHV2c4QohO0Y7U9y0AHwYwbGanAXwewBcBPG5mnwZwEsAftnOwaq2O0yPhqLOsmy8b9JfCslH/IJdPSl1cfhsc5IkR5+fCEVYAUG+Eo86m57hUtrDI99ffx6WtPTfxiK7dN/EIsWdffCXYns/zz/mhoWFqa9R5Msv+Xp6AdNfubcH2+jyfj2KJj3FsPJzEFQDO8iHif3/nkWD7ujyXKfsjiUkrrN4cgHIPT05aiUiclUo4onVxsUr7FLrCxzLj1/1yVnR+d/8UMf1O20cRQlx36Ak/IRJFzi9Eosj5hUgUOb8QiSLnFyJROpzA09HwsFy2bj2P0Kt6uLZeT0RqMnBJaWGBa0PzC7z+n5HkiE3nUYLnx3iNue4yl38mp6epbcvWsIwGAL29YRlzx408uWQBdWrLvEJtW2/kdQi37QhHEZ48yiMSf3X6dWpr5vg4UJ6lpvEz4ci4297NJbECeCLRSsbrPM7MReoa9vGksYvz4etxfpbXULSMPC1rsRS0b0d3fiESRc4vRKLI+YVIFDm/EIki5xciUeT8QiRKR6W+QpbDhqGwvJUN8EQfFQ9HWZX7uFTmpK4eAFwYp+kHsFjhMk9XKVybLstxeWVhkUtU1RqX2Bar3JYvcNloA6kNGPuUL0aSnQ7088jJ4U08cWaZJELNuvjYb9yzi9rOn+NRfblZHv1WzIelW/ZeAkDN+LVz8iK/PiZmuIz5kZ33UNuW4fAcH3udHwuszmOsyOMydOcXIlHk/EIkipxfiESR8wuRKHJ+IRKlo6v9+UKGTRvDgSfj85O0X7knHMAzMMBz4FWrfJW9Phgu/wUAPQ2+cl+thIOFNm3i+QcHh3j5r9k5HmBUiaz2N6Of2WHbdCRQaPtNPOinq8QDWd48ziu0LVYOB9uHBnieux033UZtp87y1f7zZ49TWyNHArWKfEV/osbd4sgEvz6Kw5FAsyZ/Pwf7wnM80MvnvpAPByZdRlyP7vxCpIqcX4hEkfMLkShyfiESRc4vRKLI+YVIlHbKdT0K4PcBjLr77a22LwD4IwBjrc0+5+4/WvFgOWB9d1iLyBvPnXdx/mywvVDlZbcazgM3yl1cQrk4yfPBTU+HyyrlnOduyzmXeN51683UNjfJpa3Fi6epbfO6sIRViLzTI5GgmXI3l6/y3VwyPXMmPCf3/qsHaZ/ZKZ7v8OlnnqW2gX5eLg3D4THOZlyenWrycx6r8rx6e2/ggUnNSIBXntyDN28M50EEgOxyND1CO3f+vwBwX6D9K+5+Z+vfio4vhLi+WNH53f0pABc7MBYhRAdZzW/+z5jZITN71Mz4Y2xCiOuSK3X+rwHYA+BOACMAvsQ2NLP9ZnbQzA7OLfDfv0KIznJFzu/u59294e5NAF8HsC+y7QF33+vue3u6OxpKIISIcEXOb2ZbLvnzAQAvX53hCCE6RTtS37cAfBjAsJmdBvB5AB82szuxlDHsOIA/budgOQBdpBRSLIIpvxiWci6eeYP2qXbxklaVHI8GXFzgslEpH56ujcN8yWPTRm6rV7msODt2itq2dXNZ9HguHGlXafBzHo+UmeqNSFuY4hLhtq3bg+3/67vfon16enjEX08vn8cXn/shtd327t8Itk/YDtpntsAlZGTh8l8AcMsuHh1ZyvPchZON8Pz3r+NSX7EY3l8u1/79fEXnd/dPBZofafsIQojrEj3hJ0SiyPmFSBQ5vxCJIucXIlHk/EIkSkefunEz1Ivh0lvFIpf6hjaHpZeFi/yJwVokmu78eS6jzUxz+S1HZMpGhcthmzfwyLfxsTFq+/nP/pra7n7XTmrbecvtwfZXRnhU2dQst/Wv5yXRZi6MUNs5D0ednT3JIxLzGY/E7Onh4yj3cUmsPBiWHLt6d9I+rz77CrUNDg5QW1cvlyoXc/zc6ln4Wl03PEz79PSGIw8vR+rTnV+IRJHzC5Eocn4hEkXOL0SiyPmFSBQ5vxCJ0lGpLwdHj4Xr3WXO69axhJu9W8KyIQCMTvNkkJPnuK1a5PLKGJHm8hlPpviLX/yS2nbuuIHa7nr/b1Fbf1ekXlxPWFpcqJyhfdx43bqdu/ZQ28USv3fUSK3BcplHF/7ylzxJ57atW6htxy13U1uhJxxpNzUflm0BYH6xSm237OFJVxGR2eYbTm1ZV3j+e7r59d2sh6NPl1JstIfu/EIkipxfiESR8wuRKHJ+IRJFzi9EonQ2na4DTRJvUwJfcc43wyuYPV38s6urjwdSzG7kK/p55yWXLs6ElYrZRb7COjM/TW39Q+H9AcC2G3dSW1abobYL4+EcfhfGxmmf97wnnOcOAG6OrPa/Hsl3eOTIkWB7ocRX+yu1SGmz2+/g/XI8KOzEuXDQVaMRue/lMmoaHlpPbcYubgDW5HkS8xbu113g1zALqmrWeX7H5ejOL0SiyPmFSBQ5vxCJIucXIlHk/EIkipxfiERpp1zXdgB/CWAzgCaAA+7+VTMbAvAdADuxVLLrE+4e1plaVJHhVDNcdqkUyXFm9bAkNhgpu4V8RCbp5qWTTp7i8pV3hcd++hzPS9ffx3PPvXKMl7s6NcLz+20b5OeWs7BMtW0Ll+x23xgpMxXJrbjrVh5Qc34ynBfw6OmTtE95aCO1Pffy67zfAM/h99F7PxZsf+01vr++Pn5ddXXxYJt8JG8kqlzWLRTC71lX5Bo+duRYsL1S4fkYl9POnb8O4E/d/d0APgDgT8zsNgAPA3jS3W8G8GTrbyHEO4QVnd/dR9z9+dbrGQCHAWwDcD+Ax1qbPQbg49dqkEKIq89l/eY3s50A7gLwNIBN7j4CLH1AAODf2YQQ1x1tO7+Z9QL4HoDPujt/ZvXX++03s4NmdnB+of1HD4UQ15a2nN/MClhy/G+4+/dbzefNbEvLvgXAaKivux9w973uvrfczZ/fF0J0lhWd38wMwCMADrv7ly8xPQHgodbrhwD88OoPTwhxrWgnqu8eAA8CeMnMXmi1fQ7AFwE8bmafBnASwB+utKOZRg9+OrU3PJAZHvVUJqPsrnHZpRrJmdZwHrXV7OOltwb6w/vs3cwjARfmpqhtapLnEpyb5ZKjGT+3u977nmD7wObdtM9QZPxZeZDaevvDJaMAYN+Hw9Fv285yWXRqiivF7vz6+MS/foDatm4J5/4bGz1L+7zwAo+AnJ7hEZXYGIn4q3MZsFkgF3gpXKYOAF44GvyijflKRG5cxorO7+4/B8AyRv5O20cSQlxX6Ak/IRJFzi9Eosj5hUgUOb8QiSLnFyJROprAs2EZZgrhyDjL8QeA5hB+MrAnzyPmFsHlsMUaL8fUVeLyYYGV5XK+v1yeyzWD/Zt4PyqwAN7kMuCro+QpykkeQTi2wOd+qJtfItXI5dPVFY5Im1/gUlm5h7+ff/DA71Hbru08Iat5eD7++Qd5ROLhw4eoLRdJ7llvcptnPDqyWRgIto83+LXzxlR4f4sNft0sR3d+IRJFzi9Eosj5hUgUOb8QiSLnFyJR5PxCJEpHpT5zIKuFpYhigX8O5Txsq8/yZIUe+VzLmtxWi9R9W6iFj2cRiSfL88g3RKLzFiPBWfPNSF3Dajj6rV7lEtD5l35FbVmF522xjCeYhIXrF+YjV1yJyIMAMB2JVvsPD3EZcOtwWC7bs/sG2ucD+95HbYjIy1bgtoUGH/+5qbDthUMv0T5vTIav4YqkPiHESsj5hUgUOb8QiSLnFyJR5PxCJEpnV/tzeRT7wjnhagsLtF+erKJmGf/s6o6UTop0QxV85X5uLhwk0oh8hi7Wee45iyzMNho8zXm9wfdZICeX5XifXExpIUEnAODOL59cLnxyDQ+rAAAwX+fqxz++ynPuPf7jZ6nt33zi3mB7X4FP/vZN4eAzAHjq2Zepbfd79lHbm2Oz1Pbj514Ntp8kKgAAgKhIjUhA2HJ05xciUeT8QiSKnF+IRJHzC5Eocn4hEkXOL0SirCj1mdl2AH8JYDOAJoAD7v5VM/sCgD8CMNba9HPu/qPYvoqlInbuDpeGmpnmZa3GL4ZtzYhml5G8fwCQI0EnAFA0HjSTFcK2mRku41jGp7gRk+zAx+iRt61BAkhykXJX9SaXh5qR+4NFtMqsEA7SiQlRzSY/51jAyv/9u19S2x233hhs/+g/u5322bCZB/3U8Qa1vXGC50l8+tUT1DYyEc7JaN28VNrQug3B9vFY5NQy2tmyDuBP3f15M+sD8JyZ/aRl+4q7/9e2jyaEuG5op1bfCICR1usZMzsMYNu1HpgQ4tpyWb/5zWwngLsAPN1q+oyZHTKzR82MPxYlhLjuaNv5zawXwPcAfNbdpwF8DcAeAHdi6ZvBl0i//WZ20MwOVuZ4YgghRGdpy/nNrIAlx/+Gu38fANz9vLs33L0J4OsAgg82u/sBd9/r7ntLPf1Xa9xCiFWyovPb0pLuIwAOu/uXL2nfcslmDwDgEQ9CiOuOdlb77wHwIICXzOyFVtvnAHzKzO4E4ACOA/jjlXbkzSYqs+FyTQMDPHosK4UjmEZHx2mfixOT1FbKRyTCXKSUV1c4v1/vAJdk6nUembUQiWR0np4QyPG3LZ+R40VyyLEqZEA0zWA0Qm9xcTHYXi7zPHf5iExVqYT3BwCLBX5uf//0wWD7+997G+3TPbiF2tAdltgA4KlnD1PbyCwfYyMLl4grFEq0Tz4LR5/G5Ndf28dKG7j7zxGWZ6OavhDi+kZP+AmRKHJ+IRJFzi9Eosj5hUgUOb8QidLRBJ61ahUjp06GbTmeOHPDDbuD7YMbNtM+cxUe1Tc1ySXCnkjJKCPJOKsNLnnF5Dw419F6S7xsWI3IaABgVALi8+sRGbAZjQbktq5SWKbKF/glNz/P56pY4u9LqczP7cxEWFo+RdoBYGGBvy/Pv84TiY7y4E5UM162rZmRSFJSpg4AmiwxbOSaWo7u/EIkipxfiESR8wuRKHJ+IRJFzi9Eosj5hUiUjkp9cAdI0soTR4/SblOLYSltcJhLfZs288isfCSBZ32BS0Dzc2Eth9WlAwCzSB28SDhdrRaJwiNyHgA06uFzq1Z4tKJFovOySARk/yCPxGQRelkkoWkhIh02I2OMybq/OjsWbD984jzt83c/C0cCAsDxUX59NAp91NaM1ICkNScj54wmkfTaV/p05xciVeT8QiSKnF+IRJHzC5Eocn4hEkXOL0SidFTqa3oTc4vhyK1mntfIK5GIru4iH365yKWVwRt5LbaJ8RFqGx8Ly0YLCzzKrkhq1gFAVuTnnI/IgFmOf2Z3lYjE1sOTQdZrXAZsROS3fJGfG6u7tzA/R/vMzcVsPGQuz4eBod5wdOQj33yC9jl1mkd9Wnk9tcVqHhYi13elHvaJUhfvMzQYThqb5fl1vxzd+YVIFDm/EIki5xciUeT8QiSKnF+IRFlxtd/MugA8BaDU2v677v55M9sF4NsAhgA8D+BBd+fLxgDqtTrGx8IrqYNDfBV1dipcemvj0BDtM9zPc+ANRla+i02eR65ZmQ+2TzRip80DdPKRvHrrI+dWjKgcC/PhVfE6UVkAAMZX9KtVrmRMkNJrAFCphOuNMRUAiAcs1VnOOgCIBE9V6uEV+BMk4AcA8t2RAB0WhAMgEt+FPPj4G41wYFIpcmuukPfTWcBPgHbu/BUAH3H392KpHPd9ZvYBAH8O4CvufjOACQCfbvuoQog1Z0Xn9yXeup0UWv8cwEcAfLfV/hiAj1+TEQohrglt/eY3s6xVoXcUwE8AvAlg0t3f+k57GsC2azNEIcS1oC3nd/eGu98J4AYA+wC8O7RZqK+Z7Tezg2Z2sFmL/O4UQnSUy1rtd/dJAD8D8AEAg2b21srTDQCC1Qzc/YC773X3vbkCX4QTQnSWFZ3fzDaY2WDrdTeAjwI4DOCnAP6gtdlDAH54rQYphLj6tBPYswXAY7ZU7ykH4HF3/z9m9iqAb5vZfwbwjwAeWXFPZrScVCFSrqtG8s/NTl6gferDvDxS3bjUl4/IXqV8WMvJOZfzSl38WBY5VqMZlsqAeJq2Wi3cb3p6gvaZmrhIbdOzPNjGIt/kyuVysD0m9eUiAUseKUPlDa6xVavhfrnI9VaP5c4Dt5nzXIKoc1uZeGEpEtxVIcFksfldzorO7+6HANwVaD+Gpd//Qoh3IHrCT4hEkfMLkShyfiESRc4vRKLI+YVIFItJKFf9YGZjAE60/hwGwJOldQ6N4+1oHG/nnTaOHe6+oZ0ddtT533Zgs4PuvndNDq5xaBwah772C5Eqcn4hEmUtnf/AGh77UjSOt6NxvJ1/suNYs9/8Qoi1RV/7hUiUNXF+M7vPzF43s6Nm9vBajKE1juNm9pKZvWBmBzt43EfNbNTMXr6kbcjMfmJmR1r/r1ujcXzBzM605uQFM/tYB8ax3cx+amaHzewVM/t3rfaOzklkHB2dEzPrMrNnzOzF1jj+rNW+y8yebs3Hd8wsUqisDdy9o/8AZFhKA7YbQBHAiwBu6/Q4WmM5DmB4DY77IQB3A3j5krb/AuDh1uuHAfz5Go3jCwD+fYfnYwuAu1uv+wC8AeC2Ts9JZBwdnRMABqC39boA4GksJdB5HMAnW+3/HcC/Xc1x1uLOvw/AUXc/5kupvr8N4P41GMea4e5PAVgeRH8/lhKhAh1KiErG0XHcfcTdn2+9nsFSspht6PCcRMbRUXyJa540dy2cfxuAU5f8vZbJPx3A35rZc2a2f43G8Bab3H0EWLoIAWxcw7F8xswOtX4WXPOfH5diZjuxlD/iaazhnCwbB9DhOelE0ty1cP5QepK1khzucfe7AfwugD8xsw+t0TiuJ74GYA+WajSMAPhSpw5sZr0Avgfgs+4+3anjtjGOjs+JryJpbrushfOfBrD9kr9p8s9rjbufbf0/CuAHWNvMROfNbAsAtP4fXYtBuPv51oXXBPB1dGhOzKyAJYf7hrt/v9Xc8TkJjWOt5qR17MtOmtsua+H8zwK4ubVyWQTwSQBPdHoQZtZjZn1vvQZwL4CX472uKU9gKREqsIYJUd9ythYPoANzYmaGpRyQh939y5eYOjonbBydnpOOJc3t1ArmstXMj2FpJfVNAP9xjcawG0tKw4sAXunkOAB8C0tfH2tY+ib0aQDrATwJ4Ejr/6E1Gsf/BPASgENYcr4tHRjHb2HpK+whAC+0/n2s03MSGUdH5wTAHVhKinsISx80/+mSa/YZAEcB/BWA0mqOoyf8hEgUPeEnRKLI+YVIFDm/EIki5xciUeT8QiSKnF+IRJHzC5Eocn4hEuX/A7H22yFNP8vEAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_example(cv_images, cv_labels, example_index = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building AlexNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In 2012 a convolutional neural network called AlexNet won ImageNet competition. \n",
    "\n",
    "Go through an [original AlexNet paper](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf) to investigate the architecture. Next, investigate the [basics of Keras](https://keras.io/#keras-the-python-deep-learning-library). We will use it with TensorFlow backend.\n",
    "\n",
    "You are also encouraged to go through some CNN tutorial for Keras. There is a number of them online (for example, [this](https://elitedatascience.com/keras-tutorial-deep-learning-in-python) or [this](https://machinelearningmastery.com/handwritten-digit-recognition-using-convolutional-neural-networks-python-keras/)).\n",
    "Now, build AlexNex network with Keras for object recognition. Note that standard AlexNet works with 224x224 input images. The dataset you are going to use for this problem is 32x32."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training AlexNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use training set for training the network to recognize objects. You might want to use RMSProp optimizer to speed up the training.\n",
    "\n",
    "Convolutional networks require a lot of computing power for training. Typical setup for training CNN is to use GPU, however, in this problem you are not required to do so. CPU will be fine as well.\n",
    "\n",
    "If you are using CPU for this subproblem, training process might be slow. You can stop it manually as soon as you get meaningful results.\n",
    "\n",
    "Report the results on the training and cross-validation sets. The report should contain the training logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-06T16:53:22.720090Z",
     "start_time": "2019-03-06T16:53:22.163673Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 3, 32, 32)\n",
      "(50000, 10)\n"
     ]
    }
   ],
   "source": [
    "# preprocessing input data\n",
    "\n",
    "train_images = train_images.reshape(train_images.shape[0], 3, 32, 32)\n",
    "cv_images = cv_images.reshape(cv_images.shape[0], 3, 32, 32)\n",
    "print(train_images.shape)\n",
    "\n",
    "train_images = train_images.astype('float32')\n",
    "cv_images = cv_images.astype('float32')\n",
    "train_images /= 255\n",
    "cv_images /= 255\n",
    "\n",
    "# print(len(train_labels))\n",
    "# print (train_labels[:10])\n",
    "# Convert 1-dimensional class arrays to 10-dimensional class matrices\n",
    "Y_train = np_utils.to_categorical(train_labels, 10)\n",
    "Y_cv = np_utils.to_categorical(cv_labels, 10)\n",
    "print (Y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-06T16:53:22.731815Z",
     "start_time": "2019-03-06T16:53:22.722143Z"
    }
   },
   "outputs": [],
   "source": [
    "# try model from https://elitedatascience.com/keras-tutorial-deep-learning-in-python\n",
    "# this model is suggested for MNIST dataset actually\n",
    "def create_mnist_model(need_to_train = False):\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Convolution2D(32, 3, 3, dim_ordering=\"th\", activation='relu', input_shape=(3,32,32)))\n",
    "    model.add(Convolution2D(32, 3, 3, activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    print (model.output_shape)\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    if need_to_train == True:\n",
    "        model.fit(train_images, Y_train, \n",
    "              batch_size=32, nb_epoch=10, verbose=1)\n",
    "        save_model(model_name = 'model_for_mnist')\n",
    "    else:\n",
    "        model = load_model('model_for_mnist', model)\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-06T16:53:24.408102Z",
     "start_time": "2019-03-06T16:53:22.734955Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:6: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), activation=\"relu\", input_shape=(3, 32, 32..., data_format=\"channels_first\")`\n",
      "  \n",
      "/Applications/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:7: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), activation=\"relu\")`\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 10)\n"
     ]
    }
   ],
   "source": [
    "# mnist_model = create_mnist_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-06T16:53:44.216132Z",
     "start_time": "2019-03-06T16:53:24.412768Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 20s 2ms/step\n",
      "Test loss: 2.3164710067749024\n",
      "Test accuracy: 0.1\n",
      "['loss', 'acc']\n",
      "[2.3164710067749024, 0.1]\n"
     ]
    }
   ],
   "source": [
    "# print_model_results(mnist_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-06T16:53:44.228715Z",
     "start_time": "2019-03-06T16:53:44.221650Z"
    }
   },
   "outputs": [],
   "source": [
    "# score = model.evaluate(cv_images, Y_cv, batch_size=32, verbose=1)\n",
    "# print('Test loss:', score[0])\n",
    "# print('Test accuracy:', score[1])\n",
    "# print(model.metrics_names)\n",
    "# print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-06T16:55:29.820844Z",
     "start_time": "2019-03-06T16:55:29.817716Z"
    }
   },
   "outputs": [],
   "source": [
    "epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-06T18:10:50.843Z"
    },
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "# https://engmrk.com/alexnet-implementation-using-keras/\n",
    "\n",
    "# Instantiate an empty model\n",
    "# def create_alexnet_model(need_to_train = False):\n",
    "#     model = Sequential()\n",
    "\n",
    "#     # 1st Convolutional Layer\n",
    "#     model.add(Conv2D(filters=96, \n",
    "#                      input_shape=train_images.shape[1:], \n",
    "#                      data_format = 'channels_first', \n",
    "#                      kernel_size=(5,5), \n",
    "#                      strides=(4,4), \n",
    "#                      padding='valid'))\n",
    "#     model.add(Activation('relu'))\n",
    "#     # Max Pooling\n",
    "#     model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='valid'))\n",
    "\n",
    "#     # 2nd Convolutional Layer\n",
    "#     model.add(Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), padding='valid'))\n",
    "#     model.add(Activation('relu'))\n",
    "#     # Max Pooling\n",
    "#     model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='valid'))\n",
    "\n",
    "#     # 3rd Convolutional Layer\n",
    "#     model.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding='valid'))\n",
    "#     model.add(Activation('relu'))\n",
    "\n",
    "#     # 4th Convolutional Layer\n",
    "#     model.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding='valid'))\n",
    "#     model.add(Activation('relu'))\n",
    "\n",
    "#     # 5th Convolutional Layer\n",
    "#     model.add(Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), padding='valid'))\n",
    "#     model.add(Activation('relu'))\n",
    "#     # Max Pooling\n",
    "#     model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='valid'))\n",
    "\n",
    "#     # Passing it to a Fully Connected layer\n",
    "#     model.add(Flatten())\n",
    "#     # 1st Fully Connected Layer\n",
    "#     model.add(Dense(4096, input_shape=(32*32*3,)))\n",
    "#     model.add(Activation('relu'))\n",
    "#     # Add Dropout to prevent overfitting\n",
    "#     model.add(Dropout(0.4))\n",
    "\n",
    "#     # 2nd Fully Connected Layer\n",
    "#     model.add(Dense(4096))\n",
    "#     model.add(Activation('relu'))\n",
    "#     # Add Dropout\n",
    "#     model.add(Dropout(0.4))\n",
    "\n",
    "#     # 3rd Fully Connected Layer\n",
    "#     model.add(Dense(1000))\n",
    "#     model.add(Activation('relu'))\n",
    "#     # Add Dropout\n",
    "#     model.add(Dropout(0.4))\n",
    "\n",
    "#     # Output Layer\n",
    "#     model.add(Dense(17))\n",
    "#     model.add(Activation('softmax'))\n",
    "\n",
    "#     model.summary()\n",
    "\n",
    "#     # Compile the model\n",
    "#     model.compile(loss=keras.losses.categorical_crossentropy, optimizer='adam', metrics=[\"accuracy\"])\n",
    "    \n",
    "#     if need_to_train == True:\n",
    "#         model.fit(train_images, Y_train, \n",
    "#               batch_size=32, nb_epoch=10, verbose=1)\n",
    "#         save_model(model_name = 'alexnet_model')\n",
    "#     else:\n",
    "#         model = load_model('alexnet_model', model)\n",
    "\n",
    "#     return model\n",
    "\n",
    "\n",
    "# https://github.com/jkh911208/cswithjames/blob/master/8_CIFAR10_alexnet.py\n",
    "def create_alexnet_model(need_to_train = False):\n",
    "    # AlexNet Define the Model\n",
    "    model = Sequential()\n",
    "    # model.add(Conv2D(96, (11,11), strides=(4,4), activation='relu', padding='same', input_shape=(img_height, img_width, channel,)))\n",
    "    # for original Alexnet\n",
    "    model.add(Conv2D(96, (3,3), strides=(2,2), activation='relu', \n",
    "                     padding='same', \n",
    "                     input_shape=train_images.shape[1:], \n",
    "                     data_format = 'channels_first'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2,2)))\n",
    "    # Local Response normalization for Original Alexnet\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Conv2D(256, (5,5), activation='relu', padding='same'))\n",
    "    model.add(MaxPooling2D(pool_size=(3, 3), strides=(2,2)))\n",
    "    # Local Response normalization for Original Alexnet\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Conv2D(384, (3,3), activation='relu', padding='same'))\n",
    "    model.add(Conv2D(384, (3,3), activation='relu', padding='same'))\n",
    "    model.add(Conv2D(256, (3,3), activation='relu', padding='same'))\n",
    "    model.add(MaxPooling2D(pool_size=(3, 3), strides=(2,2)))\n",
    "    # Local Response normalization for Original Alexnet\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(4096, activation='tanh'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(4096, activation='tanh'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "\n",
    "    # print the model summary\n",
    "    model.summary()\n",
    "\n",
    "    # determine Loss function and Optimizer\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    if need_to_train == True:\n",
    "        model.fit(train_images, Y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(cv_images, Y_cv),\n",
    "                    shuffle=True)\n",
    "        save_model(model_name = 'alexnet_model')\n",
    "    else:\n",
    "        model = load_model('alexnet_model', model)\n",
    "\n",
    "    return model_name\n",
    "\n",
    "    # Train the Model\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-06T17:50:50.353108Z",
     "start_time": "2019-03-06T16:55:34.330029Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_133 (Conv2D)          (None, 96, 16, 16)        2688      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_65 (MaxPooling (None, 48, 8, 16)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 48, 8, 16)         64        \n",
      "_________________________________________________________________\n",
      "conv2d_134 (Conv2D)          (None, 48, 8, 256)        102656    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_66 (MaxPooling (None, 23, 3, 256)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 23, 3, 256)        1024      \n",
      "_________________________________________________________________\n",
      "conv2d_135 (Conv2D)          (None, 23, 3, 384)        885120    \n",
      "_________________________________________________________________\n",
      "conv2d_136 (Conv2D)          (None, 23, 3, 384)        1327488   \n",
      "_________________________________________________________________\n",
      "conv2d_137 (Conv2D)          (None, 23, 3, 256)        884992    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_67 (MaxPooling (None, 11, 1, 256)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 11, 1, 256)        1024      \n",
      "_________________________________________________________________\n",
      "flatten_31 (Flatten)         (None, 2816)              0         \n",
      "_________________________________________________________________\n",
      "dense_65 (Dense)             (None, 4096)              11538432  \n",
      "_________________________________________________________________\n",
      "dropout_76 (Dropout)         (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_66 (Dense)             (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "dropout_77 (Dropout)         (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_67 (Dense)             (None, 10)                40970     \n",
      "=================================================================\n",
      "Total params: 31,565,770\n",
      "Trainable params: 31,564,714\n",
      "Non-trainable params: 1,056\n",
      "_________________________________________________________________\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "50000/50000 [==============================] - 1823s 36ms/step - loss: 5.8419 - acc: 0.1444 - val_loss: 2.3613 - val_acc: 0.1494\n",
      "Epoch 2/50\n",
      "44576/50000 [=========================>....] - ETA: 3:01 - loss: 2.8236 - acc: 0.1753"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-248-0f5c23c987d2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0malexnet_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_alexnet_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-247-353636185082>\u001b[0m in \u001b[0;36mcreate_alexnet_model\u001b[0;34m(need_to_train)\u001b[0m\n\u001b[1;32m    122\u001b[0m                     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m                     \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m                     validation_data=(cv_images, Y_cv))\n\u001b[0m\u001b[1;32m    125\u001b[0m         \u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'alexnet_model'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "alexnet_model = create_alexnet_model(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-06T17:50:50.361574Z",
     "start_time": "2019-03-06T16:55:44.111Z"
    }
   },
   "outputs": [],
   "source": [
    "print_model_results(alexnet_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving AlexNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, AlexNet does not work very well on such a small dataset. Recall what you have learned from this class to improve its performance. You can also take a look at the [Dropout technique](https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf) and its [implementation in Keras](https://keras.io/layers/core/#dropout). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-06T17:51:04.410312Z",
     "start_time": "2019-03-06T17:51:04.389113Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_model_for_cifar(need_to_train = False):\n",
    "    batch_size = 32\n",
    "    num_classes = 10\n",
    "    epochs = 50\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), \n",
    "                     padding='same', \n",
    "                     data_format = 'channels_first', \n",
    "                     input_shape=train_images.shape[1:]))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv2D(32, (3, 3)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv2D(64, (3, 3)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_classes))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    # initiate RMSprop optimizer\n",
    "    opt = keras.optimizers.rmsprop(lr=0.0001, decay=1e-6)\n",
    "\n",
    "    # Let's train the model using RMSprop\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=opt,\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    if need_to_train == True:\n",
    "        model.fit(train_images, Y_train, \n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              validation_data=(cv_images, Y_cv),\n",
    "              shuffle=True)\n",
    "        save_model(model_name = 'model_for_cifar')\n",
    "    else:\n",
    "        model = load_model('model_for_cifar', model)\n",
    "    \n",
    "    return model\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-06T17:51:05.428Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "50000/50000 [==============================] - 186s 4ms/step - loss: 2.1000 - acc: 0.2266 - val_loss: 1.9415 - val_acc: 0.3046\n",
      "Epoch 2/50\n",
      "50000/50000 [==============================] - 177s 4ms/step - loss: 1.8915 - acc: 0.3196 - val_loss: 1.7655 - val_acc: 0.3684\n",
      "Epoch 3/50\n",
      "50000/50000 [==============================] - 176s 4ms/step - loss: 1.7520 - acc: 0.3682 - val_loss: 1.6453 - val_acc: 0.3996\n",
      "Epoch 4/50\n",
      "50000/50000 [==============================] - 176s 4ms/step - loss: 1.6443 - acc: 0.4086 - val_loss: 1.5697 - val_acc: 0.4316\n",
      "Epoch 5/50\n",
      "50000/50000 [==============================] - 176s 4ms/step - loss: 1.5791 - acc: 0.4313 - val_loss: 1.4963 - val_acc: 0.4641\n",
      "Epoch 6/50\n",
      "50000/50000 [==============================] - 176s 4ms/step - loss: 1.5277 - acc: 0.4529 - val_loss: 1.4633 - val_acc: 0.4682\n",
      "Epoch 7/50\n",
      "50000/50000 [==============================] - 182s 4ms/step - loss: 1.4889 - acc: 0.4670 - val_loss: 1.4534 - val_acc: 0.4810\n",
      "Epoch 8/50\n",
      "50000/50000 [==============================] - 178s 4ms/step - loss: 1.4569 - acc: 0.4803 - val_loss: 1.4151 - val_acc: 0.4938\n",
      "Epoch 9/50\n",
      "50000/50000 [==============================] - 174s 3ms/step - loss: 1.4310 - acc: 0.4894 - val_loss: 1.4136 - val_acc: 0.4913\n",
      "Epoch 10/50\n",
      "50000/50000 [==============================] - 176s 4ms/step - loss: 1.4081 - acc: 0.4973 - val_loss: 1.3738 - val_acc: 0.5047\n",
      "Epoch 11/50\n",
      "50000/50000 [==============================] - 177s 4ms/step - loss: 1.3887 - acc: 0.5058 - val_loss: 1.3612 - val_acc: 0.5213\n",
      "Epoch 12/50\n",
      "50000/50000 [==============================] - 178s 4ms/step - loss: 1.3646 - acc: 0.5136 - val_loss: 1.3293 - val_acc: 0.5259\n",
      "Epoch 13/50\n",
      "50000/50000 [==============================] - 182s 4ms/step - loss: 1.3507 - acc: 0.5207 - val_loss: 1.3269 - val_acc: 0.5246\n",
      "Epoch 14/50\n",
      "50000/50000 [==============================] - 189s 4ms/step - loss: 1.3341 - acc: 0.5285 - val_loss: 1.3151 - val_acc: 0.5342\n",
      "Epoch 15/50\n",
      "50000/50000 [==============================] - 188s 4ms/step - loss: 1.3223 - acc: 0.5298 - val_loss: 1.3231 - val_acc: 0.5201\n",
      "Epoch 16/50\n",
      "17216/50000 [=========>....................] - ETA: 1:54 - loss: 1.3055 - acc: 0.5402"
     ]
    }
   ],
   "source": [
    "cifar_model = create_model_for_cifar(need_to_train = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-06T17:50:50.381657Z",
     "start_time": "2019-03-06T16:55:59.453Z"
    }
   },
   "outputs": [],
   "source": [
    "print_model_results(cifar_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
