{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport keras\nimport matplotlib.pyplot as plt\nfrom keras.layers import Dense,GlobalAveragePooling2D\nfrom keras.applications import MobileNet\nfrom keras.preprocessing import image\nfrom keras.applications.mobilenet import preprocess_input\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.models import Model\nfrom keras.optimizers import Adam\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\nfrom sklearn.model_selection import train_test_split\nfrom glob import glob \n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport keras\n\nimport cv2\n\nimport imgaug as ia\nfrom imgaug import augmenters as iaa\nfrom tensorflow.keras.preprocessing.image import img_to_array as img_to_array\nfrom tensorflow.keras.preprocessing.image import load_img as load_img\n\ndef readImage(path, target_size):   \n    return img_to_array(load_img(path, target_size=target_size))/255.\n\nclass DataGenerator(keras.utils.Sequence):\n    'Generates data for Keras'\n    def __init__(self, list_IDs, labels, batch_size=32, dim=(96,96), n_channels=1,\n                 n_classes=10, shuffle=True, augmentation = True):\n        'Initialization'\n    \n        self.dim = dim\n        self.batch_size = batch_size\n        self.labels = labels\n        self.list_IDs = list_IDs\n        self.n_channels = n_channels\n        self.n_classes = n_classes\n        self.shuffle = shuffle\n        self.augmentation = augmentation\n\n        num = self.labels.shape[0] - (self.labels.shape[0] % batch_size)\n        self.classes = self.labels.iloc[:num].values\n        if (self.augmentation == True):\n            print(\"Data augmentation enabled\")\n\n        self.on_epoch_end()\n\n    def __len__(self):\n        'Denotes the number of batches per epoch'\n        \n        return int(np.floor(len(self.list_IDs) / self.batch_size))\n\n    def __getitem__(self, index):\n        'Generate one batch of data'\n        # Generate indexes of the batch\n        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n       \n        list_IDs_temp = [self.list_IDs.iloc[k] for k in indexes]\n        list_labels_temp = [self.labels.iloc[k] for k in indexes]\n\n        # Generate data\n        X, y = self.__data_generation(list_IDs_temp, list_labels_temp)\n\n        return X, y\n\n    def on_epoch_end(self):\n        'Updates indexes after each epoch'\n        self.indexes = np.arange(len(self.list_IDs))\n        if self.shuffle == True:\n            np.random.shuffle(self.indexes)\n\n    def __data_generation(self, list_IDs_temp, list_labels_temp):\n        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n        # Initialization\n        \n        X = np.empty((self.batch_size, *self.dim, self.n_channels), dtype=np.uint8)\n        y = np.empty((self.batch_size), dtype=int)\n        # Generate data\n\n        for i, ID in enumerate(list_IDs_temp):\n            # Store sample\n            path = \"../input/train/\" + ID + \".tif\"\n            X[i,] = readImage(path,self.dim)\n\n            # Store class\n            y[i] = list_labels_temp[i]\n\n        if (self.augmentation == True):\n            X = self.augment(X)\n        # y_out = keras.utils.to_categorical(y, num_classes=self.n_classes)\n    \n        return X, y\n\n    def augment(self, images, show = False):\n        'Images augmentation'\n        # Sometimes(0.5, ...) applies the given augmenter in 50% of all cases,\n        # e.g. Sometimes(0.5, GaussianBlur(0.3)) would blur roughly every second image.\n        sometimes = lambda aug: iaa.Sometimes(0.5, aug)\n\n        # Define our sequence of augmentation steps that will be applied to every image\n        # All augmenters with per_channel=0.5 will sample one value _per image_\n        # in 50% of all cases. In all other cases they will sample new values\n        # _per channel_.\n        seq = iaa.Sequential(\n            [\n                # apply the following augmenters to most images\n                iaa.Fliplr(0.5), # horizontally flip 50% of all images\n                iaa.Flipud(0.2), # vertically flip 20% of all images\n                # crop images by -5% to 10% of their height/width\n                sometimes(iaa.CropAndPad(\n                    percent=(-0.05, 0.1),\n                    pad_mode=ia.ALL,\n                    pad_cval=(0, 255)\n                )),\n                sometimes(iaa.Affine(\n                    scale={\"x\": (0.8, 1.2), \"y\": (0.8, 1.2)}, # scale images to 80-120% of their size, individually per axis\n                    translate_percent={\"x\": (-0.2, 0.2), \"y\": (-0.2, 0.2)}, # translate by -20 to +20 percent (per axis)\n                    rotate=(-45, 45), # rotate by -45 to +45 degrees\n                    shear=(-16, 16), # shear by -16 to +16 degrees\n                    order=[0, 1], # use nearest neighbour or bilinear interpolation (fast)\n                    cval=(0, 255), # if mode is constant, use a cval between 0 and 255\n                    mode=ia.ALL # use any of scikit-image's warping modes (see 2nd image from the top for examples)\n                )),\n                # execute 0 to 5 of the following (less important) augmenters per image\n                # don't execute all of them, as that would often be way too strong\n                iaa.SomeOf((0, 5),\n                    [\n                        sometimes(iaa.Superpixels(p_replace=(0, 1.0), n_segments=(20, 200))), # convert images into their superpixel representation\n                        iaa.OneOf([\n                            iaa.GaussianBlur((0, 3.0)), # blur images with a sigma between 0 and 3.0\n                            iaa.AverageBlur(k=(2, 7)), # blur image using local means with kernel sizes between 2 and 7\n                            iaa.MedianBlur(k=(3, 11)), # blur image using local medians with kernel sizes between 2 and 7\n                        ]),\n                        iaa.Sharpen(alpha=(0, 1.0), lightness=(0.75, 1.5)), # sharpen images\n                        iaa.Emboss(alpha=(0, 1.0), strength=(0, 2.0)), # emboss images\n                        # search either for all edges or for directed edges,\n                        # blend the result with the original image using a blobby mask\n                        iaa.SimplexNoiseAlpha(iaa.OneOf([\n                            iaa.EdgeDetect(alpha=(0.5, 1.0)),\n                            iaa.DirectedEdgeDetect(alpha=(0.5, 1.0), direction=(0.0, 1.0)),\n                        ])),\n                        iaa.AdditiveGaussianNoise(loc=0, scale=(0.0, 0.05*255), per_channel=0.5), # add gaussian noise to images\n                        iaa.OneOf([\n                            iaa.Dropout((0.01, 0.1), per_channel=0.5), # randomly remove up to 10% of the pixels\n                            iaa.CoarseDropout((0.03, 0.15), size_percent=(0.02, 0.05), per_channel=0.2),\n                        ]),\n                        iaa.Invert(0.05, per_channel=True), # invert color channels\n                        iaa.Add((-10, 10), per_channel=0.5), # change brightness of images (by -10 to 10 of original value)\n                        iaa.AddToHueAndSaturation((-20, 20)), # change hue and saturation\n                        # either change the brightness of the whole image (sometimes\n                        # per channel) or change the brightness of subareas\n                        iaa.OneOf([\n                            iaa.Multiply((0.5, 1.5), per_channel=0.5),\n                            iaa.FrequencyNoiseAlpha(\n                                exponent=(-4, 0),\n                                first=iaa.Multiply((0.5, 1.5), per_channel=True),\n                                second=iaa.ContrastNormalization((0.5, 2.0))\n                            )\n                        ]),\n                        iaa.ContrastNormalization((0.5, 2.0), per_channel=0.5), # improve or worsen the contrast\n                        iaa.Grayscale(alpha=(0.0, 1.0)),\n                        sometimes(iaa.ElasticTransformation(alpha=(0.5, 3.5), sigma=0.25)), # move pixels locally around (with random strengths)\n                        sometimes(iaa.PiecewiseAffine(scale=(0.01, 0.05))), # sometimes move parts of the image around\n                        sometimes(iaa.PerspectiveTransform(scale=(0.01, 0.1)))\n                    ],\n                    random_order=True\n                )\n            ],\n            random_order=True\n        )\n        \n        if (show == True):\n            # show an image with 8*8 augmented versions of image 0\n            seq.show_grid(images[0], cols=8, rows=8)\n        \n        return seq.augment_images(images)\n\n    #def classes(self):\n     #   return self.labels.iloc[:].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TRAINING_RATIO = 0.9\nIMAGE_SIZE = 128\nEPOCHS = 6\nBATCH_SIZE = 192\nVERBOSITY = 1\nTESTING_BATCH_SIZE = 5000","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndata = pd.read_csv('../input/train_labels.csv')\nprint(data['label'].value_counts())\ntraining_data, validation_data = train_test_split(data, train_size=TRAINING_RATIO, stratify=data['label'])\nprint(training_data['label'].value_counts())\nprint(validation_data['label'].value_counts())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {'dim': (IMAGE_SIZE, IMAGE_SIZE),\n          'batch_size': BATCH_SIZE,\n          'n_classes': 2,\n          'n_channels': 3,\n          'shuffle': True,\n          'augmentation': False}\n\ntraining_generator = DataGenerator(training_data.id, training_data.label, **params)\nvalidation_generator = DataGenerator(validation_data.id, validation_data.label, **params)\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"base_model=MobileNet(weights='imagenet',include_top=False,\n                    input_shape=(128,128,3)) #imports the mobilenet model and discards the last 1000 neuron layer.\n\nx=base_model.output\nx=GlobalAveragePooling2D()(x)\nx=Dense(1024,activation='relu')(x) #we add dense layers so that the model can learn more complex functions and classify for better results.\nx=Dense(1024,activation='relu')(x) #dense layer 2\nx=Dense(512,activation='relu')(x) #dense layer 3\npreds=Dense(1,activation='softmax')(x) #final layer with softmax activation","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model=Model(inputs=base_model.input,outputs=preds)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for layer in model.layers[:20]:\n    layer.trainable=False\nfor layer in model.layers[20:]:\n    layer.trainable=True\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(optimizer='Adam',loss='binary_crossentropy',metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit_generator(generator=training_generator,\n                    # steps_per_epoch=2,\n                    validation_data=validation_generator, \n                    #validation_steps=3,\n                    use_multiprocessing=True,\n                    workers=6,\n                    epochs=EPOCHS)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"testing_files = glob(os.path.join('../input/test/','*.tif'))\nsubmission = pd.DataFrame()\nfor index in range(0, len(testing_files), TESTING_BATCH_SIZE):\n    data_frame = pd.DataFrame({'path': testing_files[index:index+TESTING_BATCH_SIZE]})\n    data_frame['id'] = data_frame.path.map(lambda x: x.split('/')[3].split(\".\")[0])\n    data_frame['image'] = data_frame['path'].map(lambda x: readImage(x, (IMAGE_SIZE,IMAGE_SIZE)))\n    images = np.stack(data_frame.image, axis=0)\n    predicted_labels = [model.predict(np.expand_dims(image, axis=0))[0][0] for image in images]\n    predictions = np.array(predicted_labels)\n    data_frame['label'] = predictions\n    submission = pd.concat([submission, data_frame[[\"id\", \"label\"]]])\nsubmission.to_csv(\"mobilenet_custom-aug.csv\", index=False, header=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}